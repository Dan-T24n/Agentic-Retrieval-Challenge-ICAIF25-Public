{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACM ICAIF-25 AI Agentic Retrieval Grand Challenge â€“ Baseline\n",
    "\n",
    "This notebook provides a **complete baseline solution** for the ranking evaluation tasks using **Databricks GPT OSS 120B** with a financial analyst system prompt.\n",
    "\n",
    "**Note**: this notebook works directly on Kaggle paths. To work locally, we need to resolve paths with `config.py`.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The competition consists of two main ranking tasks:\n",
    "\n",
    "1. **Document Ranking** â€“ Identify and rank the five most relevant documents.  \n",
    "2. **Chunk Ranking** â€“ Identify and rank the five most relevant text chunks.\n",
    "\n",
    "---\n",
    "\n",
    "## Processing Pipeline\n",
    "\n",
    "- **Data Loading**  \n",
    "  Reads evaluation data from JSONL files.\n",
    "\n",
    "- **Token Analysis**  \n",
    "  Checks input size to decide whether it exceed the context length of the model..\n",
    "\n",
    "- **Smart Ranking**  \n",
    "  - *Normal cases*: Single-stage ranking.  \n",
    "  - *High-token cases*: Multi-stage divide-and-conquer ranking to handle large inputs efficiently.\n",
    "\n",
    "- **Result Compilation**  \n",
    "  Combines model outputs and prepares a final CSV submission file.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- **kaggle_submission.csv**  \n",
    "  Ready-to-submit file containing the required `sample_id` and `target_index` columns.\n",
    "\n",
    "- **Comprehensive Statistics**  \n",
    "  Summarized metrics and analysis of ranking results.\n",
    "\n",
    "- **Top-5 Rankings**  \n",
    "  Returns the five most relevant items for each query as required by the challenge.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "This notebook enables **end-to-end evaluation**: from loading data to generating a Kaggle-ready submission file.  \n",
    "Simply run the pipeline and upload the generated `kaggle_submission.csv` to the competition platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:08.903921Z",
     "start_time": "2025-09-15T14:24:07.666005Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install openai tiktoken python-dotenv pydantic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.205131Z",
     "start_time": "2025-09-15T14:24:08.907592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from typing import Dict, List\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Model Setup\n",
    "Search for â€œDatabricks free trialâ€ as shown in the screenshot and sign up.\n",
    "\n",
    "1. Use Model Serving to set up a Databricks ***model endpoint***,\n",
    "2. Prepare an access token from the ***Settings page***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.31413Z",
     "start_time": "2025-09-15T14:24:09.280526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clients initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Environment variables\n",
    "DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Check if environment variables are set\n",
    "if not DATABRICKS_TOKEN:\n",
    "    print(\"âš ï¸ DATABRICKS_TOKEN not found in environment variables\")\n",
    "    print(\"Please set your Databricks token in your environment or .env file\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"âš ï¸ OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"Please set your OpenAI API key in your environment or .env file\")\n",
    "\n",
    "# Initialize clients\n",
    "client = AsyncOpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"PLEASE_COPY_AND_PASTE_YOUR_DATABRICKS_MODEL_ENDPOINT\"\n",
    ")\n",
    "\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "print(\"âœ… Clients initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic Models for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.326383Z",
     "start_time": "2025-09-15T14:24:09.324122Z"
    }
   },
   "outputs": [],
   "source": [
    "class Format(BaseModel):\n",
    "    answer: List[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `load_evaluation_data`\n",
    "\n",
    "Loads a JSONL file and returns a list of dictionaries.\n",
    "\n",
    "- Opens the file and parses each line as JSON.  \n",
    "- Handles missing files or parsing errors gracefully (prints error and returns `[]`).  \n",
    "- Prints the number of items successfully loaded.\n",
    "\n",
    "Useful for quickly reading evaluation datasets stored in JSONL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.341734Z",
     "start_time": "2025-09-15T14:24:09.339093Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_evaluation_data(filepath: str) -> List[Dict]:\n",
    "    \"\"\"Load evaluation data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    total_count = 0\n",
    "    print(f\"ğŸ“ Loading data from: {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                total_count += 1\n",
    "                item = json.loads(line.strip())\n",
    "                data.append(item)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found: {filepath}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"âœ… Total items loaded: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `create_chunk_prompt_top_k`\n",
    "\n",
    "This function builds a **prompt string** that asks a language model to pick and rank the most relevant text chunks for a given question.\n",
    "\n",
    "**Key Steps**\n",
    "1. **Determine Top-k Size**  \n",
    "   - Uses `k` (default 10) if there are more than 10 chunks.  \n",
    "   - Otherwise uses the total number of chunks.\n",
    "\n",
    "2. **Compose Prompt**  \n",
    "   - Starts with instructions to select and rank the `actual_k` most relevant chunks.  \n",
    "   - Inserts the **question** and all provided text chunks, each labeled with its original index.\n",
    "\n",
    "3. **Specify Output Format**  \n",
    "   - Requests a final ordered list of chunk indices, e.g.  \n",
    "     `[1st_most_relevant_index, 2nd_most_relevant_index, ..., kth_most_relevant_index]`.\n",
    "\n",
    "**Purpose**  \n",
    "This prompt can be sent to a large language model to **identify and rank the most relevant pieces of text** (chunks) before downstream tasks like question answering or summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.349394Z",
     "start_time": "2025-09-15T14:24:09.347004Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_chunk_prompt_top_k(question: str, chunks: List[str], chunk_indices: List[int], k: int = 10) -> str:\n",
    "    \"\"\"Ask model to select and rank only top-k most relevant chunks\"\"\"\n",
    "    # Use k if chunks length > 10, else use chunks length\n",
    "    actual_k = k if len(chunks) > 10 else len(chunks)\n",
    "    \n",
    "    prompt = f\"\"\"Identify the {actual_k} most relevant text chunks for answering this question, then rank them in order of relevance (best first).\n",
    "Question: {question}\n",
    "Text chunks:\n",
    "\"\"\"\n",
    "    for i, (chunk, orig_idx) in enumerate(zip(chunks, chunk_indices)):\n",
    "        prompt += f\"[Chunk Index {orig_idx}] {chunk}\\n\"\n",
    "    prompt += f\"\"\"\n",
    "Task: Select and rank the {actual_k} most relevant chunks among the given text chunks.\n",
    "- Put the BEST chunk first\n",
    "- Put the 2nd best chunk second  \n",
    "- Continue until you have ranked your top {actual_k} chunks\n",
    "Response Format: [1st_most_relevant_index, 2nd_most_relevant_index, ..., {actual_k}th_most_relevant_index]\"\"\"\n",
    "   \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `get_model_response`\n",
    "\n",
    "Asynchronously gets a **financial-analysis response** from a specified model and extracts a ranked list.\n",
    "\n",
    "- Prepends a **system prompt**: *\"You are a helpful financial analyst.\"*  \n",
    "- Sends combined messages to the target model (default `databricks-gpt-oss-120b`).  \n",
    "- Extracts and parses the ranking list using an OpenAI helper (`gpt-4o-mini`).  \n",
    "- Uses an `asyncio.Semaphore` to limit concurrent requests.  \n",
    "- Returns an empty list on error.\n",
    "\n",
    "Efficiently retrieves and structures ranking results from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.360568Z",
     "start_time": "2025-09-15T14:24:09.35773Z"
    }
   },
   "outputs": [],
   "source": [
    "async def get_model_response(messages: List[Dict], model: str = \"databricks-gpt-oss-120b\", semaphore: asyncio.Semaphore = None) -> List[int]:\n",
    "    \"\"\"Get response from the model with financial analyst system prompt\"\"\"\n",
    "    async with semaphore:\n",
    "        system_message = {\"role\": \"system\", \"content\": \"You are a helpful financial analyst.\"}\n",
    "        \n",
    "        full_messages = [system_message] + messages\n",
    "        \n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                messages=full_messages,\n",
    "                model=model,\n",
    "            )\n",
    "            # Get the raw text response\n",
    "            raw_output = response.choices[0].message.content[1][\"text\"].strip()\n",
    "            \n",
    "            # Use OpenAI to extract structured ranking\n",
    "            extraction_response = await openai_client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Extract the ranking list from this text. Return only the numbers in order as a list: {raw_output}\"}\n",
    "                ],\n",
    "                response_format=Format,\n",
    "            )\n",
    "            \n",
    "            return extraction_response.choices[0].message.parsed.answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"âŒ Error getting model response: {e}\")\n",
    "            # Return default ranking based on number of items expected\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `extract_ranking_from_response`\n",
    "\n",
    "Ensures the ranking list has the required length.\n",
    "\n",
    "- If `response` has at least `num_items`, return the first `num_items`.\n",
    "- Otherwise, pad with default indices (`0,1,2,...`) until the length reaches `num_items`.\n",
    "\n",
    "Guarantees a fixed-size ranking list for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.366409Z",
     "start_time": "2025-09-15T14:24:09.364506Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ranking_from_response(response: List[int], num_items: int) -> List[int]:\n",
    "    \"\"\"Extract ranking list from model response\"\"\"\n",
    "    # Ensure we have the right number of items\n",
    "    if len(response) >= num_items:\n",
    "        return response[:num_items]\n",
    "    else:\n",
    "        # Pad with default values if needed\n",
    "        padded = response + list(range(len(response), num_items))\n",
    "        return padded[:num_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `process_chunk_ranking_two_stage`\n",
    "\n",
    "Handles **chunk ranking** for prompts with very high token counts using a two-stage strategy.\n",
    "\n",
    "- **Token Check**  \n",
    "  - Uses `tiktoken` to count tokens in the first message.  \n",
    "  - If over 60,000 tokens, switches to multi-stage processing.\n",
    "\n",
    "- **Multi-Stage Processing (if large)**  \n",
    "  - Extracts `question` and `[Chunk Index N]` sections via regex.  \n",
    "  - Splits chunks into three groups (first/second/third).  \n",
    "  - For each group, calls `get_model_response` to find top candidates.  \n",
    "  - Merges these top chunks and runs a final `get_model_response` to get the final ranking.\n",
    "\n",
    "- **Single-Stage Processing (if normal)**  \n",
    "  - Directly calls `get_model_response` and extracts top-10 ranking.\n",
    "\n",
    "- **Error Handling**  \n",
    "  - Logs parsing or runtime errors and returns an empty list on failure.\n",
    "\n",
    "This approach efficiently **ranks large text sets** by first narrowing candidates in parallel and then re-ranking a smaller subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.377509Z",
     "start_time": "2025-09-15T14:24:09.371444Z"
    }
   },
   "outputs": [],
   "source": [
    "async def process_chunk_ranking_two_stage(messages: List[Dict], semaphore: asyncio.Semaphore, query_id: str) -> List[int]:\n",
    "    \"\"\"Process chunk ranking with multi-stage approach for high token count cases\"\"\"\n",
    "    try:\n",
    "        # Check if this is a high token case by examining the message content\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        content = messages[0].get('content', '')\n",
    "        token_count = len(encoding.encode(content))\n",
    "        \n",
    "        if token_count > 60000:\n",
    "            \n",
    "            # Extract question and chunks from the message content\n",
    "            # Find question\n",
    "            question_start = content.find('Question:')\n",
    "            question_end = content.find('\\n', question_start)\n",
    "            if question_start != -1 and question_end != -1:\n",
    "                question = content[question_start + len('Question:'):question_end].strip()\n",
    "            else:\n",
    "                question = None\n",
    "            \n",
    "            # Find chunks using regex-like pattern matching\n",
    "            chunks = []\n",
    "            chunk_indices = []\n",
    "            \n",
    "            import re\n",
    "            # Pattern to match [Chunk Index N] followed by content until next [Chunk Index] or Task:\n",
    "            chunk_pattern = r'\\[Chunk Index (\\d+)\\]\\s*([\\s\\S]*?)(?=\\[Chunk Index|Task:|$)'\n",
    "            matches = re.findall(chunk_pattern, content)\n",
    "            \n",
    "            for i, match in enumerate(matches):\n",
    "                orig_idx = int(match[0])\n",
    "                chunk_content = match[1].strip()\n",
    "                \n",
    "                # Clean up chunk content - remove any task instructions that might be caught\n",
    "                if 'Task:' in chunk_content:\n",
    "                    chunk_content = chunk_content.split('Task:')[0].strip()\n",
    "                \n",
    "                if chunk_content:\n",
    "                    chunks.append(chunk_content)\n",
    "                    chunk_indices.append(orig_idx)\n",
    "            \n",
    "            if not question or not chunks:\n",
    "                print(\"âš ï¸ Could not parse question and chunks, falling back to normal processing\")\n",
    "                response = await get_model_response(messages, semaphore=semaphore)\n",
    "                predicted_ranking = extract_ranking_from_response(response, 10)\n",
    "            else:\n",
    "                # Split chunks into three parts\n",
    "                third_point_1 = len(chunks) // 3\n",
    "                third_point_2 = (len(chunks) * 2) // 3\n",
    "\n",
    "                # First third\n",
    "                first_third_chunks = chunks[:third_point_1]\n",
    "                first_third_indices = chunk_indices[:third_point_1]\n",
    "                first_prompt = create_chunk_prompt_top_k(question, first_third_chunks, first_third_indices, k=10)\n",
    "                first_messages = [{\"role\": \"user\", \"content\": first_prompt}]\n",
    "                first_response = await get_model_response(first_messages, semaphore=semaphore)\n",
    "                first_top_3 = extract_ranking_from_response(first_response, 10)\n",
    "\n",
    "                # Second third\n",
    "                second_third_chunks = chunks[third_point_1:third_point_2]\n",
    "                second_third_indices = chunk_indices[third_point_1:third_point_2]\n",
    "                second_prompt = create_chunk_prompt_top_k(question, second_third_chunks, second_third_indices, k=10)\n",
    "                second_messages = [{\"role\": \"user\", \"content\": second_prompt}]\n",
    "                second_response = await get_model_response(second_messages, semaphore=semaphore)\n",
    "                second_top_3 = extract_ranking_from_response(second_response, 10)\n",
    "\n",
    "                # Third third\n",
    "                third_third_chunks = chunks[third_point_2:]\n",
    "                third_third_indices = chunk_indices[third_point_2:]\n",
    "                third_prompt = create_chunk_prompt_top_k(question, third_third_chunks, third_third_indices, k=10)\n",
    "                third_messages = [{\"role\": \"user\", \"content\": third_prompt}]\n",
    "                third_response = await get_model_response(third_messages, semaphore=semaphore)\n",
    "                third_top_4 = extract_ranking_from_response(third_response, 10)\n",
    "\n",
    "                # Combine top results from each third\n",
    "                combined_indices = first_top_3 + second_top_3 + third_top_4\n",
    "                combined_chunks = []\n",
    "\n",
    "                # Get chunks for the combined indices while preserving original indices\n",
    "                for idx in combined_indices:\n",
    "                    if idx in chunk_indices:\n",
    "                        chunk_pos = chunk_indices.index(idx)\n",
    "                        combined_chunks.append(chunks[chunk_pos])\n",
    "\n",
    "                final_prompt = create_chunk_prompt_top_k(question, combined_chunks, combined_indices, k=10)\n",
    "                final_messages = [{\"role\": \"user\", \"content\": final_prompt}]\n",
    "                final_response = await get_model_response(final_messages, semaphore=semaphore)\n",
    "                predicted_ranking = extract_ranking_from_response(final_response, 10)\n",
    "                \n",
    "        else:\n",
    "            # Normal single-stage processing\n",
    "            response = await get_model_response(messages, semaphore=semaphore)\n",
    "            predicted_ranking = extract_ranking_from_response(response, 10)\n",
    "        \n",
    "        return predicted_ranking\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"âŒ Error processing chunk ranking item: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `process_single_item`\n",
    "\n",
    "Processes one evaluation item to obtain a ranked list.\n",
    "\n",
    "- Calls `get_model_response` with the given messages.  \n",
    "- Uses `extract_ranking_from_response` to ensure the ranking has `num_items` elements.  \n",
    "- Returns an empty list if any error occurs.\n",
    "\n",
    "A simple wrapper for **single-item chunk ranking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.383807Z",
     "start_time": "2025-09-15T14:24:09.381592Z"
    }
   },
   "outputs": [],
   "source": [
    "async def process_single_item(messages: List[Dict], num_items: int, semaphore: asyncio.Semaphore) -> List[int]:\n",
    "    \"\"\"Process a single evaluation item\"\"\"\n",
    "    try:\n",
    "        # Get model response\n",
    "        response = await get_model_response(messages, semaphore=semaphore)\n",
    "        \n",
    "        # Extract ranking from response\n",
    "        predicted_ranking = extract_ranking_from_response(response, num_items)\n",
    "        \n",
    "        return predicted_ranking\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing item: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: `evaluate_chunk_ranking` & `evaluate_document_ranking`\n",
    "\n",
    "Run **end-to-end evaluation** for chunk and document ranking tasks.\n",
    "\n",
    "#### `evaluate_chunk_ranking`\n",
    "- Loads evaluation data and checks availability.\n",
    "- For each item:\n",
    "  - Uses `process_chunk_ranking_two_stage` to handle high-token cases.\n",
    "  - Collects the top 5 ranked chunk indices for submission.\n",
    "- Displays progress and summary (tasks completed, total submission entries).\n",
    "\n",
    "#### `evaluate_document_ranking`\n",
    "- Similar flow for document ranking, but:\n",
    "  - Uses `process_single_item` (no multi-stage splitting).\n",
    "  - Also records the top 5 ranked document indices.\n",
    "\n",
    "Both functions return a **submission-ready list of dictionaries** with  \n",
    "`sample_id` and `target_index` for each predicted top item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.391851Z",
     "start_time": "2025-09-15T14:24:09.38863Z"
    }
   },
   "outputs": [],
   "source": [
    "async def evaluate_chunk_ranking(data_path: str, semaphore: asyncio.Semaphore) -> List[Dict]:\n",
    "    \"\"\"Evaluate chunk ranking task and return submission data\"\"\"\n",
    "    print(\"\\nğŸ” CHUNK RANKING EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    data = load_evaluation_data(data_path)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"âŒ No data loaded for chunk ranking\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ğŸ¯ Evaluating {len(data)} chunk ranking items...\")\n",
    "    \n",
    "    # Create tasks for concurrent processing\n",
    "    tasks = []\n",
    "    submission_data = []\n",
    "    for idx, item in enumerate(data):\n",
    "        messages = item['messages']\n",
    "        query_id = item['_id']  # Use original _id from data\n",
    "        # Use multi-stage chunk ranking process for high token cases\n",
    "        task = process_chunk_ranking_two_stage(messages, semaphore, query_id)\n",
    "        tasks.append((task, query_id))\n",
    "    \n",
    "    # Process all tasks with progress bar\n",
    "    results_list = []\n",
    "    for task_tuple in tqdm(tasks, desc=\"ğŸ”„ Processing chunk ranking\", leave=False):\n",
    "        task, query_id = task_tuple\n",
    "        result = await task\n",
    "        if result:\n",
    "            results_list.append((result, query_id))\n",
    "            # Add top 5 results to submission data\n",
    "            for rank, doc_idx in enumerate(result[:5]):\n",
    "                submission_data.append({'sample_id': query_id, 'target_index': doc_idx})\n",
    "    \n",
    "    print(f\"âœ… Completed {len(results_list)} chunk ranking tasks\")\n",
    "    print(f\"ğŸ“Š Generated {len(submission_data)} submission entries\")\n",
    "    return submission_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.400051Z",
     "start_time": "2025-09-15T14:24:09.397011Z"
    }
   },
   "outputs": [],
   "source": [
    "async def evaluate_document_ranking(data_path: str, semaphore: asyncio.Semaphore) -> List[Dict]:\n",
    "    \"\"\"Evaluate document ranking task and return submission data\"\"\"\n",
    "    print(\"\\nğŸ“„ DOCUMENT RANKING EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    data = load_evaluation_data(data_path)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"âŒ No data loaded for document ranking\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ğŸ¯ Evaluating {len(data)} document ranking items...\")\n",
    "    \n",
    "    # Create tasks for concurrent processing\n",
    "    tasks = []\n",
    "    submission_data = []\n",
    "    for idx, item in enumerate(data):\n",
    "        messages = item['messages']\n",
    "        query_id = item['_id']  # Use original _id from data\n",
    "        task = process_single_item(messages, 10, semaphore)\n",
    "        tasks.append((task, query_id))\n",
    "    \n",
    "    # Process all tasks with progress bar\n",
    "    results_list = []\n",
    "    for task_tuple in tqdm(tasks, desc=\"ğŸ”„ Processing document ranking\", leave=False):\n",
    "        task, query_id = task_tuple\n",
    "        result = await task\n",
    "        if result:\n",
    "            results_list.append((result, query_id))\n",
    "            # Add top 5 results to submission data\n",
    "            for rank, doc_idx in enumerate(result[:5]):\n",
    "                submission_data.append({'sample_id': query_id, 'target_index': doc_idx})\n",
    "    \n",
    "    print(f\"âœ… Completed {len(results_list)} document ranking tasks\")\n",
    "    print(f\"ğŸ“Š Generated {len(submission_data)} submission entries\")\n",
    "    return submission_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `save_submission_csv`\n",
    "\n",
    "Saves prediction results to a **CSV file** in the required format.\n",
    "\n",
    "- Creates a CSV with header: `sample_id, target_index`.  \n",
    "- Writes each entry from `submission_data` as a row.  \n",
    "- Prints the output file path and total entry count.  \n",
    "- Logs an error message if file writing fails.\n",
    "\n",
    "Convenient for generating **final submission files** from evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.41377Z",
     "start_time": "2025-09-15T14:24:09.411397Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_submission_csv(submission_data: List[Dict], filename: str):\n",
    "    \"\"\"Save submission data to CSV file in the required format\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['sample_id', 'target_index'])\n",
    "            \n",
    "            for entry in submission_data:\n",
    "                writer.writerow([entry['sample_id'], entry['target_index']])\n",
    "        \n",
    "        print(f\"ğŸ’¾ Submission file saved to {filename}\")\n",
    "        print(f\"ğŸ“Š Total entries: {len(submission_data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving submission file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data File Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.425571Z",
     "start_time": "2025-09-15T14:24:09.422315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking for required data files...\n",
      "ğŸ“ Chunk ranking file: ./output/chunk_ranking_kaggle_eval.jsonl\n",
      "   Exists: âŒ\n",
      "ğŸ“ Document ranking file: ./output/document_ranking_kaggle_eval.jsonl\n",
      "   Exists: âŒ\n",
      "\n",
      "âš ï¸ Missing required data files. Please ensure both files exist in the ./output/ directory.\n",
      "   You may need to run the data preparation script first.\n"
     ]
    }
   ],
   "source": [
    "# Check if data files exist\n",
    "chunk_ranking_path = \"./output/chunk_ranking_kaggle_eval.jsonl\"\n",
    "document_ranking_path = \"./output/document_ranking_kaggle_eval.jsonl\"\n",
    "\n",
    "print(\"ğŸ” Checking for required data files...\")\n",
    "print(f\"ğŸ“ Chunk ranking file: {chunk_ranking_path}\")\n",
    "print(f\"   Exists: {'âœ…' if os.path.exists(chunk_ranking_path) else 'âŒ'}\")\n",
    "print(f\"ğŸ“ Document ranking file: {document_ranking_path}\")\n",
    "print(f\"   Exists: {'âœ…' if os.path.exists(document_ranking_path) else 'âŒ'}\")\n",
    "\n",
    "if os.path.exists(chunk_ranking_path) and os.path.exists(document_ranking_path):\n",
    "    print(\"\\nğŸ‰ All required files found! Ready to run evaluation.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Missing required data files. Please ensure both files exist in the ./output/ directory.\")\n",
    "    print(\"   You may need to run the data preparation script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T14:24:09.43278Z",
     "start_time": "2025-09-15T14:24:09.429721Z"
    }
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main evaluation function\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ† KAGGLE RANKING EVALUATION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ¤– Model: databricks-gpt-oss-120b\")\n",
    "    print(\"ğŸ‘¨â€ğŸ’¼ System Prompt: You are a helpful financial analyst.\")\n",
    "    print(\"ğŸ“„ Output: CSV submission file only\")\n",
    "    print(\"ğŸ”„ Concurrency: 1 simultaneous request\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create semaphore for limiting concurrent requests\n",
    "    semaphore = asyncio.Semaphore(1)\n",
    "    \n",
    "    # Check files exist before starting\n",
    "    if not os.path.exists(chunk_ranking_path) or not os.path.exists(document_ranking_path):\n",
    "        print(\"âŒ Required data files not found. Please check the file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Evaluate chunk ranking and document ranking concurrently\n",
    "    print(\"\\nğŸš€ Starting evaluation...\")\n",
    "    chunk_task = evaluate_chunk_ranking(chunk_ranking_path, semaphore)\n",
    "    doc_task = evaluate_document_ranking(document_ranking_path, semaphore)\n",
    "    \n",
    "    # Wait for both evaluations to complete\n",
    "    print(\"\\nâ³ Running both evaluations concurrently...\")\n",
    "    chunk_submission, doc_submission = await asyncio.gather(chunk_task, doc_task)\n",
    "    \n",
    "    # Combine submission data\n",
    "    all_submission_data = chunk_submission + doc_submission\n",
    "    \n",
    "    # Save submission CSV\n",
    "    save_submission_csv(all_submission_data, './kaggle_submission.csv')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸŠ EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ğŸ” Chunk ranking entries: {len(chunk_submission):,}\")\n",
    "    print(f\"ğŸ“„ Document ranking entries: {len(doc_submission):,}\")\n",
    "    print(f\"ğŸ“Š Total submission entries: {len(all_submission_data):,}\")\n",
    "    print(f\"ğŸ’¾ Submission file: kaggle_submission.csv\")\n",
    "    print(\"\\nğŸš€ Ready for Kaggle submission!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Run the Complete Evaluation\n",
    "\n",
    "Execute this cell to run the full evaluation pipeline. Make sure you have:\n",
    "\n",
    "1. âœ… Set up your environment variables (DATABRICKS_TOKEN, OPENAI_API_KEY)\n",
    "2. âœ… Have the evaluation data files in the `./output/` directory:\n",
    "   - `chunk_ranking_kaggle_eval.jsonl`\n",
    "   - `document_ranking_kaggle_eval.jsonl`\n",
    "3. âœ… Installed all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:52:41.945268Z",
     "start_time": "2025-09-15T14:24:09.441914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ† KAGGLE RANKING EVALUATION PIPELINE\n",
      "============================================================\n",
      "ğŸ¤– Model: databricks-gpt-oss-120b\n",
      "ğŸ‘¨â€ğŸ’¼ System Prompt: You are a helpful financial analyst.\n",
      "ğŸ“„ Output: CSV submission file only\n",
      "ğŸ”„ Concurrency: 1 simultaneous request\n",
      "============================================================\n",
      "âŒ Required data files not found. Please check the file paths.\n"
     ]
    }
   ],
   "source": [
    "# Run the complete evaluation pipeline\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š View Results\n",
    "\n",
    "Check the generated submission file and its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:52:42.666017Z",
     "start_time": "2025-09-15T15:52:41.995441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Submission file not found. Please run the evaluation first.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display submission file if it exists\n",
    "submission_file = './kaggle_submission.csv'\n",
    "if os.path.exists(submission_file):\n",
    "    df = pd.read_csv(submission_file)\n",
    "    print(f\"ğŸ“Š Submission file shape: {df.shape}\")\n",
    "    print(f\"\\nğŸ“‹ Sample data (first 10 rows):\")\n",
    "    print(df.head(10))\n",
    "    print(f\"\\nğŸ¯ Statistics:\")\n",
    "    print(f\"   â€¢ Unique sample_ids: {df['sample_id'].nunique():,}\")\n",
    "    print(f\"   â€¢ Sample ID range: {df['sample_id'].min()} to {df['sample_id'].max()}\")\n",
    "    print(f\"   â€¢ Target index range: {df['target_index'].min()} to {df['target_index'].max()}\")\n",
    "    print(f\"   â€¢ Total entries: {len(df):,}\")\n",
    "    \n",
    "    # Show distribution of entries per sample_id\n",
    "    entries_per_sample = df.groupby('sample_id').size()\n",
    "    print(f\"\\nğŸ“ˆ Entries per sample_id distribution:\")\n",
    "    print(f\"   â€¢ Mean: {entries_per_sample.mean():.1f}\")\n",
    "    print(f\"   â€¢ Min: {entries_per_sample.min()}\")\n",
    "    print(f\"   â€¢ Max: {entries_per_sample.max()}\")\n",
    "    print(f\"   â€¢ Most common: {entries_per_sample.mode().iloc[0]} entries per sample\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Submission file not found. Please run the evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Ready for Competition!**\n",
    "The generated `kaggle_submission.csv` file can be directly uploaded to the Kaggle competition platform.\n",
    "\n",
    "**Good luck! ğŸ†**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "agentic-rag-challenge25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
