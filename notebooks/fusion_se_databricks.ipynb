{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACM ICAIF 2025: Fusion Split-Ensemble Model\n",
    "\n",
    "Solution to the *Agentic Retrieval Grand Challenge, ACM-ICAIF '25*.\n",
    "\n",
    "**Databricks Reproducibility Notebook**\n",
    "\n",
    "### Model Architecture\n",
    "Three-stage split ensemble for chunk ranking:\n",
    "- Stage 1 (120B): Local ranking with BM25 fusion, normalized within-split <- Recall\n",
    "- Stage 2 (120B): Split ~50 candidates into 2x25, pure LLM ranking <- Recall\n",
    "- Stage 3 (405B): Final rescore on ~20 candidates, pure LLM -> Precision\n",
    "\n",
    "Architecture for both Document and Chunk ranking:\n",
    "- Smart retry with ensemble, any task/stage -> emulate multiple judges\n",
    "    - Adaptive retries: based on response quality, fuse multiple partial answers\n",
    "    - Forced retries: redo same query again to get more opinions, then fusion\n",
    "- 4-level semaphore: QUERY → STAGE1_PART → STAGE2_PART → STAGE3\n",
    "- 4-level parsing: text blocks → reasoning → regex → GPT-4o-mini rescue\n",
    "- Max-5 chunk splitting with pre-processing at data load\n",
    "\n",
    "### Core ideas\n",
    "- Manage attention allocation, avoid long context, avoid crowded chunk-pool\n",
    "- Stage-specific custom strategy: prompting + model choice + ensemble\n",
    "\n",
    "**Model design**: Read more [docs/model_fusion_se.md](https://github.com/yourusername/AgentRAG_Public/blob/main/docs/model_fusion_se.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "1. *Install required packages*\n",
    "2. *Set environment variables*\n",
    "3. *Download competition data*\n",
    "4. *Update paths*: in Configuration cell (DATA_DIR, OUTPUT_DIR) if needed\n",
    "5. *Run all cells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install bm25s pandas jsonlines tqdm httpx tiktoken kaggle python-dotenv\n",
    "\n",
    "# Restart Python kernel\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Databricks: set your keys in using secrets\n",
    "\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "# w = WorkspaceClient()\n",
    "# w.secrets.put_secret(\n",
    "#     \"icaif\",\n",
    "#     \"databricks_token\",\n",
    "#     string_value=\"your_token_here\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Loaded environment variables from .env file\n",
      "\n",
      "Environment variables configured:\n",
      "  DATABRICKS_TOKEN: 36 characters\n",
      "  OPENAI_API_KEY: 164 characters\n",
      "  KAGGLE_USERNAME: pandalikematcha\n",
      "  KAGGLE_KEY: 32 characters\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "import os\n",
    "\n",
    "# Detect environment: Databricks or local\n",
    "try:\n",
    "    # Try to access dbutils - only available in Databricks\n",
    "    dbutils.secrets.get(\"icaif\", \"databricks_token\")\n",
    "    is_databricks = True\n",
    "    print(\"Running in Databricks environment\")\n",
    "except:\n",
    "    is_databricks = False\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "if is_databricks:\n",
    "    # Use Databricks secrets (recommended for production)\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = dbutils.secrets.get(\"icaif\", \"databricks_token\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\"icaif\", \"openai_api_key\")\n",
    "    \n",
    "    # Kaggle credentials (for data download)\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = dbutils.secrets.get(\"icaif\", \"kaggle_username\")\n",
    "    os.environ[\"KAGGLE_KEY\"] = dbutils.secrets.get(\"icaif\", \"kaggle_key\")\n",
    "else:\n",
    "    # Load from .env file for local execution\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded environment variables from .env file\")\n",
    "\n",
    "# Verify environment variables are set\n",
    "print(\"\\nEnvironment variables configured:\")\n",
    "print(f\"  DATABRICKS_TOKEN: {len(os.environ.get('DATABRICKS_TOKEN', ''))} characters\")\n",
    "print(f\"  OPENAI_API_KEY: {len(os.environ.get('OPENAI_API_KEY', ''))} characters\")\n",
    "print(f\"  KAGGLE_USERNAME: {os.environ.get('KAGGLE_USERNAME', 'NOT SET')}\")\n",
    "print(f\"  KAGGLE_KEY: {len(os.environ.get('KAGGLE_KEY', ''))} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "- Modify paths (DATA_DIR, OUTPUT_DIR) in Configuration cell below\n",
    "- Set SAMPLE_SIZE for testing (None for full production run of 400 queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbp16/Desktop/AgentRAG_Public/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import bm25s\n",
    "import httpx\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n",
      "  Models: databricks-gpt-oss-120b / databricks-meta-llama-3-1-405b-instruct / databricks-meta-llama-3-1-405b-instruct\n",
      "  Data directory: ./data/raw\n",
      "  Output directory: ./output\n",
      "  Sample size: 2\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these settings for your Databricks environment\n",
    "# =============================================================================\n",
    "\n",
    "# Model Configuration (3-stage vertical ensemble)\n",
    "MODEL_STAGE1 = \"databricks-gpt-oss-120b\"                       # Stage 1: Local ranking\n",
    "MODEL_STAGE2 = \"databricks-meta-llama-3-1-405b-instruct\"       # Stage 2: Split rescore\n",
    "MODEL_STAGE3 = \"databricks-meta-llama-3-1-405b-instruct\"       # Stage 3: Final rescore\n",
    "\n",
    "# Databricks Serving Endpoint\n",
    "DATABRICKS_SERVING_ENDPOINT = os.environ.get(\n",
    "    \"DATABRICKS_SERVING_ENDPOINT\",\n",
    "    \"https://dbc-e650c56f-0e0e.cloud.databricks.com/serving-endpoints\")\n",
    "\n",
    "# 4-Level Concurrency Control\n",
    "QUERY_SEMAPHORE = 5          # Level 1: Max concurrent queries\n",
    "STAGE1_PART_SEMAPHORE = 2    # Level 2: Max concurrent Stage 1 parts per query\n",
    "STAGE2_PART_SEMAPHORE = 2    # Level 3: Max concurrent Stage 2 parts per query\n",
    "STAGE3_SEMAPHORE = 2         # Level 4: Max concurrent Stage 3 calls\n",
    "\n",
    "# Fusion and Splitting Parameters\n",
    "FUSION_WEIGHT_STAGE1 = 0.7   # Stage 1: 70% LLM semantic, 30% BM25 lexical\n",
    "STAGE2_SPLIT_COUNT = 2       # Split 50 candidates into 2 parts (25 each)\n",
    "STAGE2_K_PER_PART = 10       # Extract top-10 from each part (~20 for Stage 3)\n",
    "TARGET_TOKENS_PER_PART = 15000  # Target for chunk-based splitting\n",
    "FIXED_LOCAL_K = 10           # Fixed K for local ranking\n",
    "\n",
    "# Timing Parameters (rate limit management)\n",
    "DOC_STAGGER_INTERVAL = 2.0   # seconds between document query starts\n",
    "CHUNK_STAGGER_INTERVAL = 3.0 # seconds between chunk query starts\n",
    "STAGE1_JITTER_MAX = 25.0     # Stage 1 jitter (per-part)\n",
    "STAGE2_JITTER_MAX = 15.0     # Stage 2 jitter (per-part)\n",
    "STAGE3_JITTER_MAX = 15.0     # Stage 3 jitter (single call)\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS - Configure for your Databricks environment\n",
    "# =============================================================================\n",
    "\n",
    "# Default paths for local testing\n",
    "DATA_DIR = \"./data/raw\"\n",
    "OUTPUT_DIR = \"./output\"\n",
    "\n",
    "# For Databricks, uncomment and modify these paths:\n",
    "# DATA_DIR = \"/dbfs/mnt/data\"\n",
    "# OUTPUT_DIR = \"/dbfs/mnt/output\"\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLE SIZE - Set to None for full production run (400 queries)\n",
    "# =============================================================================\n",
    "\n",
    "SAMPLE_SIZE = 2  # Small test sample (set to None for production)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"  Models: {MODEL_STAGE1} / {MODEL_STAGE2} / {MODEL_STAGE3}\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Sample size: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full production (400 queries)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Kaggle Data Download ===\n",
    "import os, zipfile\n",
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "def download_data(data_dir=\"./data/raw\"):\n",
    "    \"\"\"Download competition eval datasets (200 doc + 200 chunk queries)\"\"\"\n",
    "    if not os.getenv('KAGGLE_USERNAME') or not os.getenv('KAGGLE_KEY'):\n",
    "        print(\"ERROR: Set KAGGLE_USERNAME and KAGGLE_KEY environment variables\")\n",
    "        return False\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    comp = \"acm-icaif-25-ai-agentic-retrieval-grand-challenge\"\n",
    "    data_path = Path(data_dir)\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading {comp}...\")\n",
    "    api.competition_download_files(comp, path=data_path)\n",
    "    \n",
    "    # Extract\n",
    "    zip_file = data_path / f\"{comp}.zip\"\n",
    "    if zip_file.exists():\n",
    "        with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "            z.extractall(data_path)\n",
    "        zip_file.unlink()\n",
    "    \n",
    "    # Show files\n",
    "    files = sorted(data_path.glob(\"*.jsonl\"))\n",
    "    print(f\"Downloaded {len(files)} files: {[f.name for f in files]}\")\n",
    "    return True\n",
    "\n",
    "# Uncomment to run:\n",
    "# download_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APITracker:\n",
    "    \"\"\"\n",
    "    Track API statistics during model execution.\n",
    "\n",
    "    Thread-safe for concurrent async operations (CPython GIL protection).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize tracker with zero counters.\"\"\"\n",
    "        self.stats = {\n",
    "            'total_calls': 0,\n",
    "            'total_time': 0.0,\n",
    "            'document_calls': 0,\n",
    "            'document_time': 0.0,\n",
    "            'document_errors': 0,\n",
    "            'document_retries': 0,\n",
    "            'chunk_calls': 0,\n",
    "            'chunk_time': 0.0,\n",
    "            'chunk_errors': 0,\n",
    "            'chunk_retries': 0,\n",
    "            'rate_limit_retries': 0,\n",
    "            'parsing_retries': 0,\n",
    "        }\n",
    "\n",
    "    def track_call(self, call_type: str, elapsed_time: float):\n",
    "        \"\"\"Track successful API call.\"\"\"\n",
    "        self.stats['total_calls'] += 1\n",
    "        self.stats['total_time'] += elapsed_time\n",
    "\n",
    "        if call_type == 'document':\n",
    "            self.stats['document_calls'] += 1\n",
    "            self.stats['document_time'] += elapsed_time\n",
    "        elif call_type == 'chunk':\n",
    "            self.stats['chunk_calls'] += 1\n",
    "            self.stats['chunk_time'] += elapsed_time\n",
    "\n",
    "    def track_error(self, call_type: Optional[str] = None):\n",
    "        \"\"\"Track API error.\"\"\"\n",
    "        if call_type == 'document':\n",
    "            self.stats['document_errors'] += 1\n",
    "        elif call_type == 'chunk':\n",
    "            self.stats['chunk_errors'] += 1\n",
    "\n",
    "    def track_rate_limit(self):\n",
    "        \"\"\"Track rate limit retry attempt.\"\"\"\n",
    "        self.stats['rate_limit_retries'] += 1\n",
    "\n",
    "    def track_retry(self, call_type: Optional[str] = None):\n",
    "        \"\"\"Track retry attempt.\"\"\"\n",
    "        if call_type == 'document':\n",
    "            self.stats['document_retries'] += 1\n",
    "        elif call_type == 'chunk':\n",
    "            self.stats['chunk_retries'] += 1\n",
    "\n",
    "    def track_parsing_retry(self):\n",
    "        \"\"\"Track parsing error retry attempt (chunk ranking only).\"\"\"\n",
    "        self.stats['parsing_retries'] += 1\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get current statistics.\"\"\"\n",
    "        return self.stats.copy()\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics.\"\"\"\n",
    "        stats = self.get_stats()\n",
    "\n",
    "        # Compute averages\n",
    "        avg_time = stats['total_time'] / stats['total_calls'] if stats['total_calls'] > 0 else 0.0\n",
    "        avg_doc_time = stats['document_time'] / stats['document_calls'] if stats['document_calls'] > 0 else 0.0\n",
    "        avg_chunk_time = stats['chunk_time'] / stats['chunk_calls'] if stats['chunk_calls'] > 0 else 0.0\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"API STATISTICS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total API calls: {stats['total_calls']}\")\n",
    "        print(f\"Total time: {stats['total_time']:.1f}s (avg: {avg_time:.2f}s/call)\")\n",
    "        print(f\"\\nDocument ranking:\")\n",
    "        print(f\"  Calls: {stats['document_calls']}\")\n",
    "        print(f\"  Time: {stats['document_time']:.1f}s (avg: {avg_doc_time:.2f}s/call)\")\n",
    "        print(f\"  Errors: {stats['document_errors']}\")\n",
    "        print(f\"  Retries: {stats['document_retries']}\")\n",
    "        print(f\"\\nChunk ranking:\")\n",
    "        print(f\"  Calls: {stats['chunk_calls']}\")\n",
    "        print(f\"  Time: {stats['chunk_time']:.1f}s (avg: {avg_chunk_time:.2f}s/call)\")\n",
    "        print(f\"  Errors: {stats['chunk_errors']}\")\n",
    "        print(f\"  Retries: {stats['chunk_retries']}\")\n",
    "        print(f\"\\nRetries breakdown:\")\n",
    "        print(f\"  Rate limit retries: {stats['rate_limit_retries']}\")\n",
    "        print(f\"  Parsing retries: {stats['parsing_retries']}\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chunks_n_way(chunks: List[str], indices: List[int], n: int) -> List[Tuple[List[str], List[int]]]:\n",
    "    \"\"\"\n",
    "    Split chunks into N equal parts by count for multi-stage processing.\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunk texts\n",
    "        indices: List of chunk indices (same length as chunks)\n",
    "        n: Number of parts to split into\n",
    "\n",
    "    Returns:\n",
    "        List of (part_chunks, part_indices) tuples\n",
    "\n",
    "    Edge case: If len(chunks) < n, return single part\n",
    "    \"\"\"\n",
    "    # Edge case: fewer chunks than splits\n",
    "    if len(chunks) < n:\n",
    "        return [(chunks, indices)]\n",
    "\n",
    "    # Compute base chunk size per part\n",
    "    chunk_size = len(chunks) // n\n",
    "    parts = []\n",
    "\n",
    "    for i in range(n):\n",
    "        start = i * chunk_size\n",
    "\n",
    "        # Last part gets remainder\n",
    "        if i == n - 1:\n",
    "            end = len(chunks)\n",
    "        else:\n",
    "            end = start + chunk_size\n",
    "\n",
    "        # Only add non-empty parts\n",
    "        if start < len(chunks):\n",
    "            part_chunks = chunks[start:end]\n",
    "            part_indices = indices[start:end]\n",
    "            parts.append((part_chunks, part_indices))\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_chunk(chunk: str, max_chars: int = 10000) -> str:\n",
    "    \"\"\"\n",
    "    Truncate chunk preserving start, middle, end.\n",
    "\n",
    "    Total after truncation: ~8050 chars (4K + 2K + 2K + 50 marker chars)\n",
    "    \"\"\"\n",
    "    if len(chunk) <= max_chars:\n",
    "        return chunk\n",
    "\n",
    "    marker = \" [...TRUNCATED...] \"\n",
    "    first_part = chunk[:4000]\n",
    "    middle_part = chunk[4000:6000]\n",
    "    last_part = chunk[-2000:]\n",
    "\n",
    "    return first_part + marker + middle_part + marker + last_part\n",
    "\n",
    "\n",
    "def compute_bm25_scores_for_query(\n",
    "    question: str,\n",
    "    chunks: List[str],\n",
    "    chunk_indices: List[int]\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Compute BM25 scores for chunks against question using bm25s library.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping chunk_idx -> BM25 score\n",
    "        Empty dict on failure (graceful fallback)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenize with English stopwords\n",
    "        corpus_tokens = bm25s.tokenize(chunks, stopwords=\"en\")\n",
    "        query_tokens = bm25s.tokenize([question], stopwords=\"en\")\n",
    "\n",
    "        # Index and retrieve scores\n",
    "        retriever = bm25s.BM25()\n",
    "        retriever.index(corpus_tokens)\n",
    "        results, scores = retriever.retrieve(query_tokens, k=len(chunks))\n",
    "\n",
    "        # Map to dict\n",
    "        score_dict = {}\n",
    "        for retrieved_idx, score in zip(results[0], scores[0]):\n",
    "            original_idx = chunk_indices[retrieved_idx]\n",
    "            score_dict[original_idx] = float(score)\n",
    "\n",
    "        # Add missing indices with 0.0 score\n",
    "        for idx in chunk_indices:\n",
    "            if idx not in score_dict:\n",
    "                score_dict[idx] = 0.0\n",
    "\n",
    "        return score_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] BM25 computation failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def _extract_question(content: str) -> str:\n",
    "    \"\"\"Extract question text from message content.\"\"\"\n",
    "    match = re.search(r'Question:\\s*(.+?)(?:\\n|$)', content, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"Question not found in message content\")\n",
    "\n",
    "    question = match.group(1).strip()\n",
    "\n",
    "    # Remove everything after double newline\n",
    "    next_section = question.find('\\n\\n')\n",
    "    if next_section != -1:\n",
    "        question = question[:next_section].strip()\n",
    "\n",
    "    return question\n",
    "\n",
    "\n",
    "def _parse_chunks_from_message(content: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Parse chunks and indices from message content.\"\"\"\n",
    "    # Remove trailing instruction text\n",
    "    task_pattern = r'\\n+Task:\\s+Select and rank.*?Response Format:.*?$'\n",
    "    content_cleaned = re.sub(task_pattern, '', content, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Parse chunks\n",
    "    pattern = r'\\[Chunk Index (\\d+)\\]\\s*(.+?)(?=\\n\\[Chunk Index |\\n*$)'\n",
    "    matches = re.findall(pattern, content_cleaned, re.DOTALL)\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(\"No chunks found in message content\")\n",
    "\n",
    "    chunks = []\n",
    "    chunk_indices = []\n",
    "\n",
    "    for idx_str, chunk_text in matches:\n",
    "        chunk_text = chunk_text.strip()\n",
    "        if chunk_text:\n",
    "            chunks.append(chunk_text)\n",
    "            chunk_indices.append(int(idx_str))\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(\"All parsed chunks were empty\")\n",
    "\n",
    "    return chunks, chunk_indices\n",
    "\n",
    "\n",
    "def _reconstruct_message_with_truncated_chunks(\n",
    "    original_content: str,\n",
    "    chunks: List[str],\n",
    "    chunk_indices: List[int],\n",
    "    truncated_chunks: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Reconstruct message content with truncated chunks.\"\"\"\n",
    "    # Extract question section\n",
    "    pattern = r'^(.*?)\\[Chunk Index \\d+\\]'\n",
    "    match = re.search(pattern, original_content, re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        return original_content\n",
    "\n",
    "    question_section = match.group(1)\n",
    "\n",
    "    # Reconstruct chunks section\n",
    "    chunks_section = \"\"\n",
    "    for idx, truncated_chunk in zip(chunk_indices, truncated_chunks):\n",
    "        chunks_section += f\"[Chunk Index {idx}] {truncated_chunk}\\n\"\n",
    "\n",
    "    return question_section + chunks_section.rstrip()\n",
    "\n",
    "\n",
    "def _compute_splits(df: pd.DataFrame, target_tokens_per_part: int = 15000) -> pd.DataFrame:\n",
    "    \"\"\"Add n_splits column with chunk-based Max-5 strategy.\"\"\"\n",
    "    def compute_n_splits(num_chunks: int) -> int:\n",
    "        \"\"\"Compute n_splits using chunk-based Max-5 strategy.\"\"\"\n",
    "        chunks_per_part = max(30, num_chunks // 5)\n",
    "        n_splits = min(5, math.ceil(num_chunks / chunks_per_part))\n",
    "        return n_splits\n",
    "\n",
    "    df['n_splits'] = df['num_chunks'].apply(compute_n_splits)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_document_data(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load JSONL for document ranking.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: query_id, question\n",
    "    \"\"\"\n",
    "    data_records = []\n",
    "    errors = []\n",
    "    seen_query_ids = set()\n",
    "\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        for line_num, item in enumerate(reader, 1):\n",
    "            try:\n",
    "                # Extract query_id\n",
    "                if 'uuid' in item:\n",
    "                    query_id = item['uuid']\n",
    "                elif '_id' in item:\n",
    "                    query_id = item['_id']\n",
    "                elif 'record_id' in item:\n",
    "                    query_id = str(item['record_id'])\n",
    "                else:\n",
    "                    errors.append(f\"Line {line_num}: No query ID field found\")\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if query_id in seen_query_ids:\n",
    "                    errors.append(f\"Line {line_num}: Duplicate query_id '{query_id}'\")\n",
    "                    continue\n",
    "                seen_query_ids.add(query_id)\n",
    "\n",
    "                # Extract question\n",
    "                messages = item.get('messages', [])\n",
    "                if not messages:\n",
    "                    errors.append(f\"Line {line_num}: No messages field\")\n",
    "                    continue\n",
    "\n",
    "                content = messages[0].get('content', '')\n",
    "                if not content:\n",
    "                    errors.append(f\"Line {line_num}: Empty message content\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    question = _extract_question(content)\n",
    "                except ValueError as e:\n",
    "                    errors.append(f\"Line {line_num}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                if not question:\n",
    "                    errors.append(f\"Line {line_num}: Empty question after parsing\")\n",
    "                    continue\n",
    "\n",
    "                data_records.append({\n",
    "                    'query_id': query_id,\n",
    "                    'question': question\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Line {line_num}: Unexpected error - {str(e)}\")\n",
    "\n",
    "    if errors:\n",
    "        error_report = \"\\n\".join(errors[:10])\n",
    "        if len(errors) > 10:\n",
    "            error_report += f\"\\n... and {len(errors) - 10} more errors\"\n",
    "        raise ValueError(f\"Document data validation failed:\\n{error_report}\")\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_chunk_data(filepath: Path, target_tokens_per_part: int = 15000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load JSONL for chunk ranking with pre-computed splits.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: query_id, question, chunks, chunk_indices,\n",
    "                                num_chunks, num_tokens, bm25_scores, n_splits\n",
    "    \"\"\"\n",
    "    data_records = []\n",
    "    errors = []\n",
    "    seen_query_ids = set()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        for line_num, item in enumerate(reader, 1):\n",
    "            try:\n",
    "                # Extract query_id\n",
    "                if 'uuid' in item:\n",
    "                    query_id = item['uuid']\n",
    "                elif '_id' in item:\n",
    "                    query_id = item['_id']\n",
    "                elif 'record_id' in item:\n",
    "                    query_id = str(item['record_id'])\n",
    "                else:\n",
    "                    errors.append(f\"Line {line_num}: No query ID field found\")\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if query_id in seen_query_ids:\n",
    "                    errors.append(f\"Line {line_num}: Duplicate query_id '{query_id}'\")\n",
    "                    continue\n",
    "                seen_query_ids.add(query_id)\n",
    "\n",
    "                # Extract messages\n",
    "                messages = item.get('messages', [])\n",
    "                if not messages:\n",
    "                    errors.append(f\"Line {line_num}: No messages field\")\n",
    "                    continue\n",
    "\n",
    "                content = messages[0].get('content', '')\n",
    "                if not content:\n",
    "                    errors.append(f\"Line {line_num}: Empty message content\")\n",
    "                    continue\n",
    "\n",
    "                # Parse question\n",
    "                try:\n",
    "                    question = _extract_question(content)\n",
    "                except ValueError as e:\n",
    "                    errors.append(f\"Line {line_num}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                if not question:\n",
    "                    errors.append(f\"Line {line_num}: Empty question after parsing\")\n",
    "                    continue\n",
    "\n",
    "                # Parse chunks and indices\n",
    "                try:\n",
    "                    chunks, chunk_indices = _parse_chunks_from_message(content)\n",
    "                except ValueError as e:\n",
    "                    errors.append(f\"Line {line_num}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                # Compute BM25 scores on ORIGINAL chunks (before truncation)\n",
    "                bm25_scores = compute_bm25_scores_for_query(question, chunks, chunk_indices)\n",
    "\n",
    "                # Truncate chunks\n",
    "                truncated_chunks = [truncate_chunk(chunk) for chunk in chunks]\n",
    "\n",
    "                # Reconstruct message with truncated chunks\n",
    "                content_truncated = _reconstruct_message_with_truncated_chunks(\n",
    "                    content, chunks, chunk_indices, truncated_chunks\n",
    "                )\n",
    "\n",
    "                # Validation\n",
    "                record_errors = []\n",
    "\n",
    "                if len(truncated_chunks) != len(chunk_indices):\n",
    "                    record_errors.append(\"chunks/indices length mismatch\")\n",
    "\n",
    "                if any(idx < 0 for idx in chunk_indices):\n",
    "                    record_errors.append(\"negative chunk indices found\")\n",
    "\n",
    "                if any(not chunk.strip() for chunk in truncated_chunks):\n",
    "                    record_errors.append(\"empty chunks found\")\n",
    "\n",
    "                # Compute token count from TRUNCATED content\n",
    "                try:\n",
    "                    num_tokens = len(encoding.encode(content_truncated))\n",
    "                except Exception:\n",
    "                    num_tokens = 0\n",
    "\n",
    "                if not (0 < num_tokens < 1_000_000):\n",
    "                    record_errors.append(f\"token count out of range: {num_tokens}\")\n",
    "\n",
    "                if record_errors:\n",
    "                    errors.append(f\"Line {line_num}: {', '.join(record_errors)}\")\n",
    "                    continue\n",
    "\n",
    "                data_records.append({\n",
    "                    'query_id': query_id,\n",
    "                    'question': question,\n",
    "                    'chunks': truncated_chunks,\n",
    "                    'chunk_indices': chunk_indices,\n",
    "                    'num_chunks': len(truncated_chunks),\n",
    "                    'num_tokens': num_tokens,\n",
    "                    'bm25_scores': bm25_scores\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Line {line_num}: Unexpected error - {str(e)}\")\n",
    "\n",
    "    if errors:\n",
    "        error_report = \"\\n\".join(errors[:10])\n",
    "        if len(errors) > 10:\n",
    "            error_report += f\"\\n... and {len(errors) - 10} more errors\"\n",
    "        raise ValueError(f\"Chunk data validation failed:\\n{error_report}\")\n",
    "\n",
    "    df = pd.DataFrame(data_records)\n",
    "\n",
    "    # Compute n_splits dynamically\n",
    "    df = _compute_splits(df, target_tokens_per_part)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document ranking system prompt\n",
    "SYSTEM_PROMPT_DOCUMENT = \"\"\"You are a helpful financial analyst. You have expertise in SEC filings.\n",
    "\n",
    "Your task is to rank 5 financial document types to answer the given question.\n",
    "\"\"\"\n",
    "\n",
    "def build_doc_messages(question: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build messages for document ranking API call.\n",
    "    Uses Dict format: {\"0\": 2, \"1\": 3, ...}\n",
    "    \"\"\"\n",
    "    user_content = f\"\"\"\n",
    "###QUESTION###\n",
    "{question}\n",
    "\n",
    "###TASK###\n",
    "Rank ALL 5 document types using the 0-4 relevance scale to answer the given question (most relevant=4).\n",
    "\n",
    "Document types to score:\n",
    "- 0: DEF 14A (Proxy Statement)\n",
    "- 1: 10-K (Annual Report)\n",
    "- 2: 10-Q (Quarterly Report)\n",
    "- 3: 8-K (Current Report)\n",
    "- 4: Earnings Call Transcript\n",
    "\n",
    "###OUTPUT FORMAT###\n",
    "Return a JSON dictionary: document-type indices as keys (strings) and scores as values (0-4).\n",
    "You MUST include ALL 5 document types in your ranking.\n",
    "\n",
    "###REAL EXAMPLES###\n",
    "Question: What is Apple's latest positioning in terms of global smartphone market share?\n",
    "Answer: {{\"0\": 2, \"1\": 3, \"2\": 0, \"3\": 1, \"4\": 4}}\n",
    "\n",
    "This means Earnings Call Transcript (key \"4\") is most relevant with score 4, then 10-K (key \"1\") with score 3, and so on.\n",
    "\n",
    "###ANSWER###\n",
    "Return a JSON dictionary of all 5 document type rankings:\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_DOCUMENT},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "\n",
    "# Chunk recall system prompt (Stage 1 and Stage 2)\n",
    "SYSTEM_PROMPT_RECALL = \"\"\"\n",
    "You are a helpful financial analyst. You have expertise in SEC filings and documents.\n",
    "Your task is to identify the MOST relevant text chunks to answer a question.\n",
    "\"\"\"\n",
    "\n",
    "def build_chunk_messages_recall(\n",
    "    question: str,\n",
    "    chunks: List[str],\n",
    "    chunk_indices: List[int],\n",
    "    k: int = 15\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Recall Focus: Local ranking with recall focus (0-4 scale).\n",
    "    Used by Stage 1 (local ranking) and Stage 2 (split rescore).\n",
    "    Uses Dict format: {\"67\": 4, \"91\": 2, ...}\n",
    "    \"\"\"\n",
    "    # Build chunks section\n",
    "    chunks_section = \"\"\n",
    "    for chunk, orig_idx in zip(chunks, chunk_indices):\n",
    "        chunks_section += f\"---\\n**Chunk Index {orig_idx}**\\n{chunk}\\n\"\n",
    "\n",
    "    user_content = f\"\"\"###QUESTION###\n",
    "{question}\n",
    "\n",
    "###TEXT CHUNKS###\n",
    "{chunks_section}\n",
    "\n",
    "###INSTRUCTIONS###\n",
    "Identify the top-{k} most relevant chunks, assign a relevance score (0-4) to each chunk: most relevant=4, least relevant=0\n",
    "\n",
    "Principles:\n",
    "- If you find less than {k} relevant chunks, just add more random chunks with 0 score.\n",
    "- If ALL chunks are NOT relevant, give back random chunks with 0 score.\n",
    "\n",
    "###OUTPUT FORMAT###\n",
    "Your answer MUST be a JSON dictionary. Your answer MUST have exactly {k} chunk indices with scores as values (0-4).\n",
    "For chunk index, use just the index number as string (e.g., \"98\").\n",
    "\n",
    "GOOD EXAMPLE: {{\"67\": 4, \"91\": 2, \"12\": 1, \"85\": 0, \"136\": 0, ...}}\n",
    "This means: chunk \"67\" is most relevant with score 4, then chunk \"91\" with score 2, and so on.\n",
    "\n",
    "BAD EXAMPLE: {{\"Chunk Index 67: 4, Chunk Index 91: 2, ...\"}}\n",
    "\n",
    "###ANSWER###\n",
    "Return the JSON dictionary of top-{k} chunk rankings:\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_RECALL},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "\n",
    "# Chunk precision system prompt (Stage 3)\n",
    "SYSTEM_PROMPT_PRECISION = \"\"\"You are a helpful financial analyst.\n",
    "Your task is to rank the top-k RELEVANT text chunks to answer a given question.\n",
    "\"\"\"\n",
    "\n",
    "def build_chunk_messages_precision(\n",
    "    question: str,\n",
    "    chunks: List[str],\n",
    "    chunk_indices: List[int],\n",
    "    k: int = 5\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Precision Focus: Global re-scoring with precision focus (0-2 scale).\n",
    "    Used by Stage 3 (final rescore).\n",
    "    Uses List format: [67, 91, 12, 85, 136]\n",
    "    \"\"\"\n",
    "    # Build chunks section\n",
    "    chunks_section = \"\"\n",
    "    for chunk, orig_idx in zip(chunks, chunk_indices):\n",
    "        chunks_section += f\"---\\n**Chunk Index {orig_idx}**\\n{chunk}\\n\"\n",
    "\n",
    "    user_content = f\"\"\"###QUESTION###\n",
    "{question}\n",
    "\n",
    "###CANDIDATES###\n",
    "{chunks_section}\n",
    "\n",
    "###INSTRUCTIONS###\n",
    "Rank top-{k} most relevant chunks in DESCENDING order of relevance (most relevance first).\n",
    "\n",
    "###OUTPUT FORMAT###\n",
    "Your answer MUST be a RANKED LIST: with exactly {k} chunk indices, most relevant item first, least relevant item last.\n",
    "Use ONLY index number as integer (89, 12, etc.)\n",
    "\n",
    "IMPORTANT: Do NOT include verbose reasoning or explanation in the answer.\n",
    "You MUST output ONLY the list as the final answer.\n",
    "\n",
    "GOOD EXAMPLE: [67, 91, 12, 85, 136]\n",
    "This means: chunk 67 is most relevant, then both chunk 91, and so on.\n",
    "\n",
    "BAD EXAMPLE: [\"Chunk Index 67\", \"Chunk Index 91\", ... ]\n",
    "\n",
    "###ANSWER###\n",
    "Return the ranked list of top-{k} chunk rankings:\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_PRECISION},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedLLMClient:\n",
    "    \"\"\"Unified LLM client supporting OpenAI and Databricks via direct HTTP calls.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend: str,\n",
    "        model: str,\n",
    "        temperature: float = 0.1,\n",
    "        max_tokens: int = 1000,\n",
    "        extra_params: Dict[str, Any] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize unified LLM client.\n",
    "\n",
    "        Args:\n",
    "            backend: 'openai' or 'databricks'\n",
    "            model: Model name\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens in response\n",
    "            extra_params: Additional parameters (e.g., {\"reasoning_effort\": \"medium\"})\n",
    "        \"\"\"\n",
    "        self.backend = backend\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.extra_params = extra_params or {}\n",
    "\n",
    "        # Detect GPT-5 models\n",
    "        self.is_gpt5 = 'gpt-5' in model.lower()\n",
    "\n",
    "        if backend == \"openai\":\n",
    "            self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            self.api_base = \"https://api.openai.com/v1\"\n",
    "        elif backend == \"databricks\":\n",
    "            self.api_key = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "            self.api_base = os.getenv(\"DATABRICKS_SERVING_ENDPOINT\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backend: {backend}\")\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(f\"API key not found for {backend}\")\n",
    "\n",
    "    async def achat(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Direct async chat completion.\n",
    "\n",
    "        Args:\n",
    "            messages: Chat messages [{\"role\": \"user\", \"content\": \"...\"}]\n",
    "\n",
    "        Returns:\n",
    "            Raw API response dict with 'choices' field\n",
    "        \"\"\"\n",
    "        if self.backend == \"openai\":\n",
    "            url = f\"{self.api_base}/chat/completions\"\n",
    "\n",
    "            # GPT-5 models have different parameters\n",
    "            if self.is_gpt5:\n",
    "                payload = {\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"max_completion_tokens\": self.max_tokens\n",
    "                }\n",
    "                payload.update(self.extra_params)\n",
    "            else:\n",
    "                payload = {\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"max_tokens\": self.max_tokens\n",
    "                }\n",
    "                payload.update(self.extra_params)\n",
    "        else:  # databricks\n",
    "            # Build full endpoint URL with model name\n",
    "            if self.api_base.endswith('/serving-endpoints'):\n",
    "                url = f\"{self.api_base}/{self.model}/invocations\"\n",
    "            elif not self.api_base.endswith('/invocations'):\n",
    "                url = f\"{self.api_base}/invocations\"\n",
    "            else:\n",
    "                url = self.api_base\n",
    "\n",
    "            payload = {\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens\n",
    "            }\n",
    "            payload.update(self.extra_params)\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        # Increased timeout to 150s for complex queries\n",
    "        try:\n",
    "            async with httpx.AsyncClient(timeout=150.0) as client:\n",
    "                response = await client.post(url, headers=headers, json=payload)\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    text = response.text\n",
    "                    raise httpx.HTTPError(f\"API error {response.status_code}: {text}\")\n",
    "\n",
    "                return response.json()\n",
    "\n",
    "        except httpx.TimeoutException:\n",
    "            raise httpx.HTTPError(f\"API request timeout after 150s (backend: {self.backend}, model: {self.model})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ParseResult:\n",
    "    \"\"\"Result from LLM response parsing with metadata.\"\"\"\n",
    "    rankings: List[Tuple[int, int]]  # (index, score) tuples\n",
    "    is_complete: bool                 # True if len >= expected_count\n",
    "    extraction_stage: str             # \"direct\" or \"rescue\"\n",
    "\n",
    "\n",
    "class ResponseParser:\n",
    "    \"\"\"\n",
    "    Parse LLM responses with 4-stage extraction strategy.\n",
    "\n",
    "    Supports two output formats:\n",
    "    1. Dict format: {\"45\": 2, \"91\": 1, ...}\n",
    "    2. List format: [45, 91, 12, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize parser.\"\"\"\n",
    "        self.rescue_client = None  # Lazy init only if needed\n",
    "\n",
    "    async def parse_rankings(\n",
    "        self,\n",
    "        response_content: Any,\n",
    "        expected_count: Optional[int] = None\n",
    "    ) -> ParseResult:\n",
    "        \"\"\"Parse rankings from LLM response content.\"\"\"\n",
    "        # Try stages 1-3: Direct extraction\n",
    "        stage = \"direct\"\n",
    "        try:\n",
    "            rankings = self._extract_direct(response_content)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Direct extraction failed: {e}\")\n",
    "\n",
    "            # Try stage 4: Rescue parsing\n",
    "            try:\n",
    "                rankings = await self._extract_rescue(response_content)\n",
    "                stage = \"rescue\"\n",
    "            except Exception as rescue_error:\n",
    "                print(f\"[ERROR] All extraction methods failed: {rescue_error}\")\n",
    "                raise ValueError(\"All extraction methods failed\")\n",
    "\n",
    "        # Check completeness\n",
    "        is_complete = (expected_count is None or len(rankings) >= expected_count)\n",
    "\n",
    "        return ParseResult(rankings, is_complete, stage)\n",
    "\n",
    "    def _extract_direct(self, content: Any) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Direct extraction with 3 priority levels.\"\"\"\n",
    "        # Handle string content\n",
    "        if isinstance(content, str):\n",
    "            return self._extract_from_text(content)\n",
    "\n",
    "        # Handle list content (Databricks format)\n",
    "        if not isinstance(content, list):\n",
    "            raise ValueError(f\"Unexpected content type: {type(content)}\")\n",
    "\n",
    "        # Priority 1: Extract from 'text' blocks\n",
    "        text_blocks = self._collect_text_blocks(content)\n",
    "        for text in text_blocks:\n",
    "            try:\n",
    "                rankings = self._extract_from_text(text)\n",
    "                return rankings\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Priority 2: Extract from 'reasoning' blocks\n",
    "        reasoning_texts = self._collect_reasoning_texts(content)\n",
    "        for text in reasoning_texts:\n",
    "            try:\n",
    "                if self._is_example_quote(text):\n",
    "                    continue\n",
    "                rankings = self._extract_from_text(text)\n",
    "                return rankings\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Priority 3: Regex-based extraction\n",
    "        rankings = self._extract_regex(content)\n",
    "        return rankings\n",
    "\n",
    "    def _collect_text_blocks(self, content: List) -> List[str]:\n",
    "        \"\"\"Collect all 'text' blocks from list content.\"\"\"\n",
    "        blocks = []\n",
    "        for item in content:\n",
    "            if isinstance(item, dict) and item.get('type') == 'text' and 'text' in item:\n",
    "                blocks.append(item['text'])\n",
    "        return blocks\n",
    "\n",
    "    def _collect_reasoning_texts(self, content: List) -> List[str]:\n",
    "        \"\"\"Collect all reasoning texts from list content.\"\"\"\n",
    "        texts = []\n",
    "        for item in content:\n",
    "            if isinstance(item, dict) and item.get('type') == 'reasoning':\n",
    "                if 'summary' in item and isinstance(item['summary'], list):\n",
    "                    for summary_item in item['summary']:\n",
    "                        if isinstance(summary_item, dict) and 'text' in summary_item:\n",
    "                            texts.append(summary_item['text'])\n",
    "        return texts\n",
    "\n",
    "    def _normalize_index_key(self, key: str) -> int:\n",
    "        \"\"\"Normalize dictionary keys to extract numeric index.\"\"\"\n",
    "        key_str = str(key).strip()\n",
    "\n",
    "        # Try direct conversion first\n",
    "        try:\n",
    "            return int(key_str)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Pattern: \"Chunk Index X\" or \"Document Index X\"\n",
    "        match = re.search(r'(?:Chunk|Document)?\\s*Index\\s+(\\d+)', key_str, re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "\n",
    "        # Fallback: extract first number\n",
    "        match = re.search(r'\\d+', key_str)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "\n",
    "        raise ValueError(f\"Cannot extract numeric index from key: {key_str}\")\n",
    "\n",
    "    def _extract_from_text(self, text: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Extract rankings from text using JSON parsing.\"\"\"\n",
    "        # Try dict format first\n",
    "        dict_start = text.find('{')\n",
    "        dict_end = text.rfind('}') + 1\n",
    "        if dict_start >= 0 and dict_end > dict_start:\n",
    "            try:\n",
    "                json_str = text[dict_start:dict_end]\n",
    "                rank_dict = json.loads(json_str)\n",
    "                if isinstance(rank_dict, dict):\n",
    "                    rankings = [(self._normalize_index_key(k), int(v)) for k, v in rank_dict.items()]\n",
    "                    return rankings\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Try list format\n",
    "        list_start = text.find('[')\n",
    "        list_end = text.rfind(']') + 1\n",
    "        if list_start >= 0 and list_end > list_start:\n",
    "            try:\n",
    "                json_str = text[list_start:list_end]\n",
    "                rank_list = json.loads(json_str)\n",
    "                if isinstance(rank_list, list):\n",
    "                    # Assign scores by position\n",
    "                    max_score = len(rank_list) - 1\n",
    "                    rankings = [(int(idx), max_score - i) for i, idx in enumerate(rank_list)]\n",
    "                    return rankings\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        raise ValueError(\"No valid JSON found in text\")\n",
    "\n",
    "    def _is_example_quote(self, text: str) -> bool:\n",
    "        \"\"\"Check if text is just quoting the example from prompt.\"\"\"\n",
    "        indicators = [\n",
    "            'Return JSON dict like',\n",
    "            'Return JSON list like',\n",
    "            'Example:',\n",
    "            'The user asks:',\n",
    "        ]\n",
    "        quote_count = sum(1 for ind in indicators if ind in text)\n",
    "        return quote_count >= 2\n",
    "\n",
    "    def _extract_regex(self, content: Any) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Regex-based extraction.\"\"\"\n",
    "        # Extract all text from content\n",
    "        if isinstance(content, list):\n",
    "            all_text = []\n",
    "            for item in content:\n",
    "                if isinstance(item, dict):\n",
    "                    if 'text' in item:\n",
    "                        all_text.append(item['text'])\n",
    "                    elif 'summary' in item and isinstance(item['summary'], list):\n",
    "                        for summary_item in item['summary']:\n",
    "                            if isinstance(summary_item, dict) and 'text' in summary_item:\n",
    "                                all_text.append(summary_item['text'])\n",
    "            combined_text = \"\\n\".join(all_text)\n",
    "        else:\n",
    "            combined_text = str(content)\n",
    "\n",
    "        # Pattern 1: List format [2, 4, 1, 3, 0]\n",
    "        list_pattern = r'\\[\\s*(\\d+(?:\\s*,\\s*\\d+)*)\\s*\\]'\n",
    "        list_matches = re.findall(list_pattern, combined_text)\n",
    "        for match in list_matches:\n",
    "            try:\n",
    "                indices = [int(x.strip()) for x in match.split(',')]\n",
    "                max_score = len(indices) - 1\n",
    "                return [(idx, max_score - i) for i, idx in enumerate(indices)]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Pattern 2: Dict format {\"2\": 4, \"4\": 3, ...}\n",
    "        if '{' in combined_text:\n",
    "            dict_start = combined_text.find('{')\n",
    "            dict_end = combined_text.find('}', dict_start) + 1\n",
    "            if dict_start >= 0 and dict_end > dict_start:\n",
    "                dict_str = combined_text[dict_start:dict_end]\n",
    "                try:\n",
    "                    rank_dict = json.loads(dict_str)\n",
    "                    if isinstance(rank_dict, dict):\n",
    "                        rankings = [(self._normalize_index_key(k), int(v)) for k, v in rank_dict.items()]\n",
    "                        return rankings\n",
    "                except:\n",
    "                    # Fallback to regex\n",
    "                    pairs = re.findall(r'\"(\\d+)\"\\s*:\\s*(\\d+)', dict_str)\n",
    "                    if pairs:\n",
    "                        rankings = [(int(k), int(v)) for k, v in pairs]\n",
    "                        return rankings\n",
    "\n",
    "        raise ValueError(\"No valid ranking patterns found with regex\")\n",
    "\n",
    "    async def _extract_rescue(self, content: Any) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Rescue parsing using GPT-4o-mini via UnifiedLLMClient.\"\"\"\n",
    "        # Lazy init rescue client (MODIFIED: Use UnifiedLLMClient instead of LlamaIndex)\n",
    "        if self.rescue_client is None:\n",
    "            self.rescue_client = UnifiedLLMClient(\n",
    "                backend=\"openai\",\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0,\n",
    "                max_tokens=100\n",
    "            )\n",
    "\n",
    "        # Extract all text from content\n",
    "        if isinstance(content, list):\n",
    "            text_parts = []\n",
    "            for item in content:\n",
    "                if isinstance(item, dict):\n",
    "                    if 'text' in item:\n",
    "                        text_parts.append(item['text'])\n",
    "                    elif 'summary' in item and isinstance(item['summary'], list):\n",
    "                        for summary_item in item['summary']:\n",
    "                            if isinstance(summary_item, dict) and 'text' in summary_item:\n",
    "                                text_parts.append(summary_item['text'])\n",
    "            content_str = \"\\n\".join(text_parts)\n",
    "        else:\n",
    "            content_str = str(content)\n",
    "\n",
    "        rescue_prompt = f\"\"\"Extract ranking information from this model output. The text may contain\n",
    "various blocks and formats.\n",
    "IMPORTANT: Focus on the LAST part of the text for the final answer.\n",
    "Return a JSON dict with indices as keys (strings) and scores as values. Keep answer to max 10 items.\n",
    "Example: {{\"45\": 2, \"91\": 1, \"12\": 0, ...}}\n",
    "\n",
    "Output:\n",
    "{content_str}\n",
    "\n",
    "Return ONLY the JSON dict:\"\"\"\n",
    "\n",
    "        # MODIFIED: Use dict format instead of ChatMessage\n",
    "        messages = [{\"role\": \"user\", \"content\": rescue_prompt}]\n",
    "        response = await self.rescue_client.achat(messages)\n",
    "        # MODIFIED: Access response via dict keys instead of response.message.content\n",
    "        rescued_text = response['choices'][0]['message']['content']\n",
    "\n",
    "        # Extract from rescued response\n",
    "        return self._extract_from_text(rescued_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ApiResult:\n",
    "    \"\"\"\n",
    "    Result from API call with ensemble data.\n",
    "\n",
    "    Attributes:\n",
    "        attempts: ALL attempt results [(idx, score), ...]\n",
    "        quality: \"complete\" | \"early_stop\" | \"max_retries\" | \"failed\"\n",
    "        n_complete: How many complete attempts\n",
    "        avg_quality: Average completeness (0.0-1.0)\n",
    "        error: Error message if failed\n",
    "    \"\"\"\n",
    "    attempts: List[List[Tuple[int, int]]]\n",
    "    quality: str\n",
    "    n_complete: int\n",
    "    avg_quality: float\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "def fuse_retry_attempts(\n",
    "    attempts: List[List[Tuple[int, int]]],\n",
    "    k: int = 60\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Fuse multiple retry attempts using ensemble ranking.\n",
    "\n",
    "    Combines three signals:\n",
    "    1. Frequency: How many attempts included this item\n",
    "    2. RRF score: Reciprocal rank fusion across attempts\n",
    "    3. Score sum: Aggregate relevance scores\n",
    "    \"\"\"\n",
    "    item_signals = defaultdict(lambda: {\n",
    "        'frequency': 0,\n",
    "        'rrf_score': 0.0,\n",
    "        'score_sum': 0\n",
    "    })\n",
    "\n",
    "    n_attempts = len(attempts)\n",
    "\n",
    "    # Accumulate signals from all attempts\n",
    "    for attempt in attempts:\n",
    "        for rank, (idx, score) in enumerate(attempt):\n",
    "            item_signals[idx]['frequency'] += 1\n",
    "            item_signals[idx]['rrf_score'] += 1 / (rank + k)\n",
    "            item_signals[idx]['score_sum'] += score\n",
    "\n",
    "    # Calculate ensemble score\n",
    "    fused = []\n",
    "    for idx, signals in item_signals.items():\n",
    "        freq_weight = signals['frequency'] / n_attempts\n",
    "        rrf_normalized = signals['rrf_score'] / n_attempts\n",
    "        score_avg = signals['score_sum'] / signals['frequency']\n",
    "\n",
    "        # Weighted combination (40% consensus, 30% position, 30% relevance)\n",
    "        ensemble_score = (\n",
    "            freq_weight * 0.4 +\n",
    "            rrf_normalized * 0.3 +\n",
    "            score_avg * 0.3\n",
    "        )\n",
    "\n",
    "        fused.append((idx, ensemble_score))\n",
    "\n",
    "    # Sort by ensemble score descending\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "    return fused\n",
    "\n",
    "\n",
    "async def call_with_ensemble_retry(\n",
    "    api_func,\n",
    "    expected_count: int,\n",
    "    max_retries: int = 10,\n",
    "    min_attempts: int = 2,\n",
    "    quality_threshold: float = 0.6,\n",
    "    tracker=None,\n",
    "    call_type: str = \"chunk\",\n",
    "    force_min_attempts: bool = False,\n",
    "    jitter_max: float = 15.0\n",
    ") -> ApiResult:\n",
    "    \"\"\"\n",
    "    Retry with smart stopping and ensemble fusion.\n",
    "\n",
    "    Stop conditions:\n",
    "    1. First complete → STOP (unless force_min_attempts)\n",
    "    2. Early stop (n_attempts >= min AND avg_quality >= threshold)\n",
    "    3. Max retries exhausted\n",
    "    \"\"\"\n",
    "    all_attempts = []\n",
    "    n_complete = 0\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Retry jitter (skip first attempt)\n",
    "            if attempt > 0:\n",
    "                pre_jitter = random.uniform(0.0, jitter_max)\n",
    "                await asyncio.sleep(pre_jitter)\n",
    "\n",
    "            # Call API function\n",
    "            parsed = await api_func()\n",
    "\n",
    "            # Check if complete\n",
    "            if len(parsed) >= expected_count:\n",
    "                all_attempts.append(parsed)\n",
    "                n_complete += 1\n",
    "\n",
    "                # Check force_min_attempts\n",
    "                if force_min_attempts and len(all_attempts) < min_attempts:\n",
    "                    continue\n",
    "\n",
    "                # Stop condition met\n",
    "                avg_quality = 1.0\n",
    "                return ApiResult(all_attempts, \"complete\", n_complete, avg_quality)\n",
    "            else:\n",
    "                # Incomplete - store and check early stop\n",
    "                all_attempts.append(parsed)\n",
    "\n",
    "                # Calculate average quality\n",
    "                avg_quality = sum(len(a) for a in all_attempts) / (len(all_attempts) * expected_count)\n",
    "\n",
    "                # Early stop check\n",
    "                if len(all_attempts) >= min_attempts and avg_quality >= quality_threshold:\n",
    "                    return ApiResult(all_attempts, \"early_stop\", 0, avg_quality)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "\n",
    "            # Track rate limits\n",
    "            if '429' in error_str or 'rate limit' in error_str:\n",
    "                if tracker:\n",
    "                    tracker.track_rate_limit()\n",
    "\n",
    "            # Track retries\n",
    "            if tracker and attempt > 0:\n",
    "                tracker.track_retry(call_type)\n",
    "\n",
    "            print(f\"[WARNING] Attempt {attempt + 1} failed: {str(e)[:140]}\")\n",
    "\n",
    "            # Backoff + jitter\n",
    "            if attempt < max_retries - 1:\n",
    "                backoff = min(35, (attempt + 1) * 12.0)\n",
    "                jitter = random.uniform(0.0, 20.0)\n",
    "                wait_total = backoff + jitter\n",
    "                await asyncio.sleep(wait_total)\n",
    "\n",
    "            continue\n",
    "\n",
    "    # Max retries exhausted\n",
    "    avg_quality = sum(len(a) for a in all_attempts) / (len(all_attempts) * expected_count) if all_attempts else 0.0\n",
    "\n",
    "    if all_attempts:\n",
    "        return ApiResult(all_attempts, \"max_retries\", 0, avg_quality)\n",
    "\n",
    "    # No successful attempts\n",
    "    return ApiResult([], \"failed\", 0, 0.0, error=\"No successful attempts\")\n",
    "\n",
    "\n",
    "async def create_ranking(\n",
    "    api_result: ApiResult,\n",
    "    expected_count: int,\n",
    "    candidate_pool: List[int],\n",
    "    messages: Any,\n",
    "    rescue_client,\n",
    "    parser,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    domain_fallback: Optional[List[int]] = None\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Create final ranking with complete-first cascade and LLM rescue.\n",
    "\n",
    "    Fallback cascade:\n",
    "    1. Has complete → Use last complete attempt\n",
    "    2. Has incomplete (>=2 attempts) → Ensemble fusion\n",
    "    3. Failed → Try GPT-4o-mini rescue\n",
    "    4. Rescue failed + domain → Domain fallback\n",
    "    5. Rescue failed + no domain → Random\n",
    "    \"\"\"\n",
    "    # Case 1: Has complete result\n",
    "    if api_result.n_complete >= 1:\n",
    "        complete_attempt = next(\n",
    "            a for a in reversed(api_result.attempts)\n",
    "            if len(a) >= expected_count\n",
    "        )\n",
    "\n",
    "        # Sort by score descending\n",
    "        sorted_items = sorted(complete_attempt[:expected_count], key=lambda x: (-x[1], x[0]))\n",
    "        return [idx for idx, _ in sorted_items]\n",
    "\n",
    "    # Case 2: Has incomplete data - ensemble fusion\n",
    "    if len(api_result.attempts) >= 2:\n",
    "        fused = fuse_retry_attempts(api_result.attempts)\n",
    "\n",
    "        # Sort by score descending\n",
    "        sorted_items = sorted(fused[:expected_count], key=lambda x: (-x[1], x[0]))\n",
    "        return [idx for idx, _ in sorted_items]\n",
    "\n",
    "    # Case 3: Failed - try GPT-4o-mini rescue\n",
    "    if rescue_client:\n",
    "        try:\n",
    "            async with semaphore:\n",
    "                response = await rescue_client.achat(messages)\n",
    "                content = response['choices'][0]['message']['content']\n",
    "                result = await parser.parse_rankings(content, expected_count=expected_count)\n",
    "\n",
    "                if len(result.rankings) >= expected_count:\n",
    "                    # Sort by score descending\n",
    "                    sorted_items = sorted(result.rankings[:expected_count], key=lambda x: (-x[1], x[0]))\n",
    "                    return [idx for idx, _ in sorted_items]\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] LLM rescue failed: {e}\")\n",
    "\n",
    "    # Case 4: Domain fallback\n",
    "    if domain_fallback:\n",
    "        return domain_fallback[:expected_count]\n",
    "\n",
    "    # Case 5: Random\n",
    "    candidate_pool_copy = candidate_pool.copy()\n",
    "    random.shuffle(candidate_pool_copy)\n",
    "    return candidate_pool_copy[:expected_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_n_indices(ranking: List[int], target_n: int, all_indices: List[int], query_id: str = \"\") -> List[int]:\n",
    "    \"\"\"\n",
    "    Pad ranking to exactly target_n indices.\n",
    "\n",
    "    Args:\n",
    "        ranking: Current ranking list\n",
    "        target_n: Target length\n",
    "        all_indices: Available indices for padding\n",
    "        query_id: Query ID for logging\n",
    "\n",
    "    Returns:\n",
    "        Padded ranking of exactly target_n length\n",
    "\n",
    "    Strategy: unused indices → cyclic repeats → default fallback\n",
    "    \"\"\"\n",
    "    if len(ranking) >= target_n:\n",
    "        return ranking[:target_n]\n",
    "\n",
    "    # Use unused indices first\n",
    "    used = set(ranking)\n",
    "    remaining = sorted([idx for idx in all_indices if idx not in used])\n",
    "    ranking.extend(remaining[:target_n - len(ranking)])\n",
    "\n",
    "    # Cyclic padding if still short\n",
    "    while len(ranking) < target_n and all_indices:\n",
    "        ranking.append(all_indices[len(ranking) % len(all_indices)])\n",
    "\n",
    "    # Catastrophic fallback\n",
    "    if len(ranking) < target_n:\n",
    "        print(f\"[WARNING] [{query_id}] Padding fallback: insufficient indices\")\n",
    "        while len(ranking) < target_n:\n",
    "            ranking.append(len(ranking))\n",
    "\n",
    "    return ranking[:target_n]\n",
    "\n",
    "\n",
    "def fuse_llm_bm25_scores(\n",
    "    llm_scores: List[Tuple[int, int]],\n",
    "    bm25_scores: Dict[int, float],\n",
    "    indices: List[int],\n",
    "    weight_llm: float\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Weighted fusion of LLM semantic and BM25 lexical scores.\n",
    "\n",
    "    Args:\n",
    "        llm_scores: (chunk_idx, llm_score) tuples from API\n",
    "        bm25_scores: Pre-computed BM25 scores dict\n",
    "        indices: Chunk indices in this part\n",
    "        weight_llm: LLM weight (0-1), BM25 gets (1-weight_llm)\n",
    "\n",
    "    Returns:\n",
    "        Sorted (chunk_idx, fused_score) tuples\n",
    "    \"\"\"\n",
    "    llm_dict = {idx: score for idx, score in llm_scores}\n",
    "\n",
    "    # Normalize LLM (handle all-zero case)\n",
    "    llm_max = max(llm_dict.values()) if llm_dict and any(llm_dict.values()) else 1\n",
    "    llm_norm = {idx: score / llm_max for idx, score in llm_dict.items()}\n",
    "\n",
    "    # Normalize BM25 for this part\n",
    "    part_bm25 = {idx: bm25_scores.get(idx, 0) for idx in indices}\n",
    "    bm25_max = max(part_bm25.values()) if any(part_bm25.values()) else 1\n",
    "    bm25_norm = {idx: score / bm25_max for idx, score in part_bm25.items()}\n",
    "\n",
    "    # Weighted fusion\n",
    "    weight_bm25 = 1.0 - weight_llm\n",
    "    if any(bm25_norm.values()):\n",
    "        fused = {\n",
    "            idx: weight_llm * llm_norm.get(idx, 0) + weight_bm25 * bm25_norm.get(idx, 0)\n",
    "            for idx in indices\n",
    "        }\n",
    "    else:\n",
    "        fused = llm_norm\n",
    "\n",
    "    # Scale back to preserve magnitude, sort descending\n",
    "    fused_items = [(idx, fused.get(idx, 0) * llm_max) for idx in indices]\n",
    "    fused_items.sort(key=lambda x: -x[1])\n",
    "\n",
    "    return fused_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rescore_split_stage2(\n",
    "    question: str,\n",
    "    chunks: List[str],\n",
    "    candidate_items: List[Tuple[int, float]],\n",
    "    query_id: str,\n",
    "    client_stage2,\n",
    "    parser,\n",
    "    tracker\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Stage 2: Split top-50 into 2x25 parts, rank independently with pure LLM.\n",
    "\n",
    "    Args:\n",
    "        question: User question\n",
    "        chunks: Full chunk list\n",
    "        candidate_items: (chunk_idx, stage1_score) tuples from Stage 1\n",
    "        query_id: Query identifier\n",
    "        client_stage2: Stage 2 LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "\n",
    "    Returns:\n",
    "        ~20 (chunk_idx, llm_score) tuples (top-10 from each part), or empty if failed\n",
    "\n",
    "    Sequential split preserves Stage 1 ordering quality.\n",
    "    \"\"\"\n",
    "    top_n_candidates = min(50, len(candidate_items))\n",
    "    candidates = candidate_items[:top_n_candidates]\n",
    "\n",
    "    # Sequential split (no shuffle)\n",
    "    part_size = len(candidates) // STAGE2_SPLIT_COUNT\n",
    "    parts = []\n",
    "    for i in range(STAGE2_SPLIT_COUNT):\n",
    "        start = i * part_size\n",
    "        end = start + part_size if i < STAGE2_SPLIT_COUNT - 1 else len(candidates)\n",
    "        parts.append(candidates[start:end])\n",
    "\n",
    "    stage2_part_semaphore = asyncio.Semaphore(STAGE2_PART_SEMAPHORE)\n",
    "\n",
    "    # Rank parts in parallel\n",
    "    part_tasks = []\n",
    "    for i, part_candidates in enumerate(parts):\n",
    "        part_indices = [idx for idx, _ in part_candidates]\n",
    "        part_chunks = [chunks[idx] for idx in part_indices if idx < len(chunks)]\n",
    "\n",
    "        messages = build_chunk_messages_recall(\n",
    "            question, part_chunks, part_indices, k=STAGE2_K_PER_PART\n",
    "        )\n",
    "\n",
    "        task = rank_chunk_part_with_retry(\n",
    "            messages=messages,\n",
    "            semaphore=stage2_part_semaphore,\n",
    "            query_id=f\"{query_id}_stage2_part{i+1}\",\n",
    "            expected_count=STAGE2_K_PER_PART,\n",
    "            candidate_indices=part_indices,\n",
    "            client=client_stage2,\n",
    "            parser=parser,\n",
    "            tracker=tracker,\n",
    "            rescue_client=None,  # Will be passed from caller\n",
    "            jitter_max=STAGE2_JITTER_MAX,\n",
    "            stage_name=f\"Stage2_Part{i+1}\",\n",
    "            force_min_attempts=False\n",
    "        )\n",
    "        part_tasks.append(task)\n",
    "\n",
    "    # Gather and concatenate (no sorting - Stage 3 will handle)\n",
    "    all_stage2_items = []\n",
    "    for task in part_tasks:\n",
    "        llm_scored = await task\n",
    "        all_stage2_items.extend(llm_scored[:STAGE2_K_PER_PART])\n",
    "\n",
    "    return all_stage2_items\n",
    "\n",
    "\n",
    "async def rescore_final_stage3(\n",
    "    question: str,\n",
    "    chunks: List[str],\n",
    "    candidate_items: List[Tuple[int, float]],\n",
    "    query_id: str,\n",
    "    stage3_semaphore: asyncio.Semaphore,\n",
    "    client_stage3,\n",
    "    parser,\n",
    "    tracker,\n",
    "    rescue_client\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Stage 3: Final rescore on ~20 candidates with 405B model.\n",
    "\n",
    "    Args:\n",
    "        question: User question\n",
    "        chunks: Full chunk list\n",
    "        candidate_items: (chunk_idx, stage2_score) tuples from Stage 2\n",
    "        query_id: Query identifier\n",
    "        stage3_semaphore: Independent semaphore for Stage 3\n",
    "        client_stage3: Stage 3 LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "        rescue_client: Rescue LLM client\n",
    "\n",
    "    Returns:\n",
    "        Top-10 (chunk_idx, llm_score) tuples, or empty if failed\n",
    "\n",
    "    Escapes Local Pool Paradox by assigning NEW scores in full global context.\n",
    "    Independent semaphore allows tuning 405B concurrency separately from 120B.\n",
    "    \"\"\"\n",
    "    candidates = candidate_items[:min(20, len(candidate_items))]\n",
    "\n",
    "    candidate_indices = [idx for idx, _ in candidates]\n",
    "    candidate_chunks = [chunks[i] for i in candidate_indices if i < len(chunks)]\n",
    "\n",
    "    messages = build_chunk_messages_precision(question, candidate_chunks, candidate_indices, k=10)\n",
    "\n",
    "    llm_scored_items = await rank_chunk_part_with_retry(\n",
    "        messages=messages,\n",
    "        semaphore=stage3_semaphore,\n",
    "        query_id=f\"{query_id}_stage3\",\n",
    "        expected_count=10,\n",
    "        candidate_indices=candidate_indices,\n",
    "        client=client_stage3,\n",
    "        parser=parser,\n",
    "        tracker=tracker,\n",
    "        rescue_client=rescue_client,\n",
    "        jitter_max=STAGE3_JITTER_MAX,\n",
    "        stage_name=\"Stage3\",\n",
    "        force_min_attempts=False\n",
    "    )\n",
    "\n",
    "    if not llm_scored_items:\n",
    "        llm_scored_items = candidates[:10]\n",
    "        print(f\"[WARNING] [{query_id}_stage3] FALLBACK: Using Stage 2 results\")\n",
    "\n",
    "    return llm_scored_items if llm_scored_items else []\n",
    "\n",
    "\n",
    "async def rank_chunk_part_with_retry(\n",
    "    messages: List[Dict],\n",
    "    semaphore: Optional[asyncio.Semaphore],\n",
    "    query_id: str,\n",
    "    expected_count: int,\n",
    "    candidate_indices: List[int],\n",
    "    client,\n",
    "    parser,\n",
    "    tracker,\n",
    "    rescue_client,\n",
    "    jitter_max: float = None,\n",
    "    stage_name: str = \"Stage1\",\n",
    "    force_min_attempts: bool = False\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Rank chunk part with unified smart retry strategy.\n",
    "\n",
    "    Args:\n",
    "        messages: Prompt messages\n",
    "        semaphore: Rate limiting semaphore\n",
    "        query_id: Query identifier\n",
    "        expected_count: Expected item count\n",
    "        candidate_indices: Candidate chunk indices\n",
    "        client: LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "        rescue_client: Rescue LLM client\n",
    "        jitter_max: Max jitter seconds\n",
    "        stage_name: Stage name for logging\n",
    "        force_min_attempts: Force min attempts if first complete\n",
    "\n",
    "    Returns:\n",
    "        (index, score) tuples with actual LLM scores\n",
    "\n",
    "    Handles API failures, incomplete responses, and ensemble fusion.\n",
    "    \"\"\"\n",
    "    if jitter_max is None:\n",
    "        jitter_max = STAGE1_JITTER_MAX\n",
    "\n",
    "    async def wrapped_part_call():\n",
    "        # Jitter for desynchronization (stacks with retry jitter)\n",
    "        jitter = random.uniform(0, jitter_max)\n",
    "        await asyncio.sleep(jitter)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if semaphore:\n",
    "            async with semaphore:\n",
    "                response = await client.achat(messages)\n",
    "        else:\n",
    "            response = await client.achat(messages)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if tracker:\n",
    "            tracker.track_call('chunk', elapsed)\n",
    "\n",
    "        if not response or 'choices' not in response or not response['choices']:\n",
    "            raise ValueError(\"Malformed API response - missing 'choices'\")\n",
    "\n",
    "        content = response['choices'][0]['message']['content']\n",
    "\n",
    "        result = await parser.parse_rankings(content, expected_count=expected_count)\n",
    "\n",
    "        return result.rankings\n",
    "\n",
    "    api_result = await call_with_ensemble_retry(\n",
    "        wrapped_part_call,\n",
    "        expected_count=expected_count,\n",
    "        max_retries=10,\n",
    "        min_attempts=2,\n",
    "        quality_threshold=0.7,\n",
    "        tracker=tracker,\n",
    "        call_type=\"chunk\",\n",
    "        force_min_attempts=force_min_attempts\n",
    "    )\n",
    "\n",
    "    # Return ACTUAL scores from API (not positional scores)\n",
    "    # Case 1: Complete result\n",
    "    if api_result.n_complete >= 1:\n",
    "        complete_attempt = next(\n",
    "            a for a in reversed(api_result.attempts)\n",
    "            if len(a) >= expected_count\n",
    "        )\n",
    "\n",
    "        sorted_items = sorted(complete_attempt[:expected_count], key=lambda x: (-x[1], x[0]))\n",
    "        return sorted_items\n",
    "\n",
    "    # Case 2: Ensemble fusion (preserves actual scores)\n",
    "    elif len(api_result.attempts) >= 2:\n",
    "        fused = fuse_retry_attempts(api_result.attempts)\n",
    "\n",
    "        sorted_items = sorted(fused[:expected_count], key=lambda x: (-x[1], x[0]))\n",
    "        return sorted_items\n",
    "\n",
    "    # Case 3: Rescue fallback (ONLY case using positional scores)\n",
    "    else:\n",
    "        sem_for_rescue = semaphore if semaphore else asyncio.Semaphore(1)\n",
    "\n",
    "        ranking = await create_ranking(\n",
    "            api_result,\n",
    "            expected_count=expected_count,\n",
    "            candidate_pool=candidate_indices,\n",
    "            messages=messages,\n",
    "            rescue_client=rescue_client,\n",
    "            parser=parser,\n",
    "            semaphore=sem_for_rescue,\n",
    "            domain_fallback=None\n",
    "        )\n",
    "\n",
    "        print(f\"[WARNING] [{query_id}] Using rescue/random with positional scores\")\n",
    "\n",
    "        scored_items = [(idx, expected_count - i) for i, idx in enumerate(ranking[:expected_count])]\n",
    "        return scored_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rank_single_document(\n",
    "    row: pd.Series,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    client_stage1,\n",
    "    parser,\n",
    "    tracker,\n",
    "    rescue_client\n",
    ") -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Process single document ranking query.\n",
    "\n",
    "    Args:\n",
    "        row: DataFrame row with query data\n",
    "        semaphore: Rate limiting semaphore\n",
    "        client_stage1: Stage 1 LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "        rescue_client: Rescue LLM client\n",
    "\n",
    "    Returns:\n",
    "        (query_id, ranking) tuple\n",
    "    \"\"\"\n",
    "    query_id = row['query_id']\n",
    "    question = row['question']\n",
    "    messages = build_doc_messages(question)\n",
    "\n",
    "    async def wrapped_api_call():\n",
    "        async with semaphore:\n",
    "            start_time = time.time()\n",
    "            response = await client_stage1.achat(messages)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if tracker:\n",
    "                tracker.track_call('document', elapsed)\n",
    "\n",
    "            if not response or 'choices' not in response or not response['choices']:\n",
    "                raise ValueError(\"Malformed API response - missing 'choices'\")\n",
    "\n",
    "            content = response['choices'][0]['message']['content']\n",
    "\n",
    "            result = await parser.parse_rankings(content, expected_count=5)\n",
    "\n",
    "            return result.rankings\n",
    "\n",
    "    api_result = await call_with_ensemble_retry(\n",
    "        wrapped_api_call,\n",
    "        expected_count=5,\n",
    "        max_retries=10,\n",
    "        min_attempts=2,\n",
    "        quality_threshold=0.8,\n",
    "        tracker=tracker,\n",
    "        call_type=\"document\"\n",
    "    )\n",
    "\n",
    "    # 10-K > 10-Q > DEF14A > 8-K > Earnings\n",
    "    DOMAIN_FALLBACK = [1, 2, 0, 3, 4]\n",
    "    ranking = await create_ranking(\n",
    "        api_result,\n",
    "        expected_count=5,\n",
    "        candidate_pool=[0, 1, 2, 3, 4],\n",
    "        messages=messages,\n",
    "        rescue_client=rescue_client,\n",
    "        parser=parser,\n",
    "        semaphore=semaphore,\n",
    "        domain_fallback=DOMAIN_FALLBACK\n",
    "    )\n",
    "\n",
    "    return (query_id, ranking)\n",
    "\n",
    "\n",
    "async def rank_single_chunk(\n",
    "    row: pd.Series,\n",
    "    stage3_semaphore: asyncio.Semaphore,\n",
    "    client_stage1,\n",
    "    client_stage2,\n",
    "    client_stage3,\n",
    "    parser,\n",
    "    tracker,\n",
    "    rescue_client,\n",
    "    local_predictions: List\n",
    ") -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    3-stage chunk ranking: Stage1 BM25 fusion → Stage2 split → Stage3 final.\n",
    "\n",
    "    Args:\n",
    "        row: DataFrame row with query data\n",
    "        stage3_semaphore: Independent semaphore for Stage 3\n",
    "        client_stage1: Stage 1 LLM client\n",
    "        client_stage2: Stage 2 LLM client\n",
    "        client_stage3: Stage 3 LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "        rescue_client: Rescue LLM client\n",
    "        local_predictions: List to store predictions\n",
    "\n",
    "    Returns:\n",
    "        (query_id, ranking) tuple\n",
    "    \"\"\"\n",
    "    query_id = row['query_id']\n",
    "    question = row['question']\n",
    "    chunks = row['chunks']\n",
    "    chunk_indices = row['chunk_indices']\n",
    "    n_splits = row['n_splits']\n",
    "    bm25_scores = row.get('bm25_scores', {})\n",
    "\n",
    "    all_scored_items_fallback = []\n",
    "\n",
    "    try:\n",
    "        # Stage 1: Local ranking with BM25 fusion\n",
    "        parts = split_chunks_n_way(chunks, chunk_indices, n_splits)\n",
    "\n",
    "        part_semaphore = asyncio.Semaphore(STAGE1_PART_SEMAPHORE)\n",
    "\n",
    "        # Process parts in parallel\n",
    "        part_inputs = []\n",
    "        tasks = []\n",
    "        for i, (part_chunks, part_indices) in enumerate(parts):\n",
    "            part_inputs.append(part_indices)\n",
    "            part_messages = build_chunk_messages_recall(\n",
    "                question, part_chunks, part_indices, k=FIXED_LOCAL_K\n",
    "            )\n",
    "\n",
    "            part_id = f\"{query_id}_part{i+1}\"\n",
    "            task = rank_chunk_part_with_retry(\n",
    "                part_messages, part_semaphore, part_id,\n",
    "                expected_count=FIXED_LOCAL_K,\n",
    "                candidate_indices=part_indices,\n",
    "                client=client_stage1,\n",
    "                parser=parser,\n",
    "                tracker=tracker,\n",
    "                rescue_client=rescue_client\n",
    "            )\n",
    "            tasks.append((task, part_id, part_indices))\n",
    "\n",
    "        # Gather LLM results\n",
    "        part_responses = []\n",
    "        for task, part_id, part_indices in tasks:\n",
    "            result = await task\n",
    "            part_responses.append(result)\n",
    "\n",
    "        # BM25 fusion for each part\n",
    "        fused_part_responses = []\n",
    "        for part_scores, part_indices in zip(part_responses, part_inputs):\n",
    "            fused_scores = fuse_llm_bm25_scores(\n",
    "                llm_scores=part_scores,\n",
    "                bm25_scores=bm25_scores,\n",
    "                indices=part_indices,\n",
    "                weight_llm=FUSION_WEIGHT_STAGE1\n",
    "            )\n",
    "            fused_part_responses.append(fused_scores)\n",
    "\n",
    "        # Combine and track\n",
    "        all_scored_items = []\n",
    "        for i, (part_scores, part_indices) in enumerate(zip(fused_part_responses, part_inputs)):\n",
    "            local_predictions.append((\n",
    "                query_id, f\"part{i+1}\", part_scores.copy(), n_splits, part_indices\n",
    "            ))\n",
    "            all_scored_items.extend(part_scores)\n",
    "\n",
    "        all_scored_items_fallback = all_scored_items.copy()\n",
    "\n",
    "        # Sort by fused score\n",
    "        all_scored_items.sort(key=lambda x: (-x[1], x[0]))\n",
    "        top_candidates = all_scored_items[:min(50, len(all_scored_items))]\n",
    "\n",
    "        try:\n",
    "            # Stage 2: Split rescore\n",
    "            stage2_items = await rescore_split_stage2(\n",
    "                question, chunks, top_candidates, query_id,\n",
    "                client_stage2, parser, tracker\n",
    "            )\n",
    "\n",
    "            if stage2_items:\n",
    "                local_predictions.append((\n",
    "                    query_id, \"stage2_split\", stage2_items.copy(), n_splits, None\n",
    "                ))\n",
    "\n",
    "            # Stage 3: Final rescore\n",
    "            if stage2_items:\n",
    "                rescored_items = await rescore_final_stage3(\n",
    "                    question, chunks, stage2_items, query_id, stage3_semaphore,\n",
    "                    client_stage3, parser, tracker, rescue_client\n",
    "                )\n",
    "            else:\n",
    "                rescored_items = []\n",
    "\n",
    "            if rescored_items:\n",
    "                local_predictions.append((\n",
    "                    query_id, \"stage3_final\", rescored_items.copy(), n_splits, None\n",
    "                ))\n",
    "\n",
    "            # Score-aware padding\n",
    "            if rescored_items and len(rescored_items) >= 5:\n",
    "                top_indices = [idx for idx, _ in rescored_items[:5]]\n",
    "            else:\n",
    "                # Augment with Stage 1 backup\n",
    "                combined = list(rescored_items) if rescored_items else []\n",
    "                stage3_indices = {idx for idx, _ in combined}\n",
    "                stage1_backup = [(idx, score) for idx, score in all_scored_items\n",
    "                                if idx not in stage3_indices]\n",
    "                combined.extend(stage1_backup)\n",
    "                combined.sort(key=lambda x: (-x[1], x[0]))\n",
    "                top_indices = [idx for idx, _ in combined[:5]]\n",
    "\n",
    "                print(f\"[WARNING] [{query_id}] Stage 3 insufficient ({len(rescored_items)}), augmented with Stage 1\")\n",
    "\n",
    "                local_predictions.append((\n",
    "                    query_id, \"stage3_final\", combined[:50].copy(), n_splits, None\n",
    "                ))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] [{query_id}] Stage 2/3 failed: {str(e)[:100]}\")\n",
    "            local_predictions.append((\n",
    "                query_id, \"stage3_final\", all_scored_items.copy(), n_splits, None\n",
    "            ))\n",
    "            top_indices = [idx for idx, _ in all_scored_items[:5]]\n",
    "\n",
    "        ranking = pad_to_n_indices(top_indices, 5, chunk_indices, query_id)\n",
    "\n",
    "        return (query_id, ranking)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catastrophic fallback: model scores → random → duplicates → defaults\n",
    "        error_msg = str(e).split(':')[0] if ':' in str(e) else str(e)\n",
    "        print(f\"[ERROR] Chunk ranking {query_id} failed: {error_msg}\")\n",
    "\n",
    "        top_indices = []\n",
    "\n",
    "        # Use model scores if available\n",
    "        if all_scored_items_fallback:\n",
    "            top_indices = [idx for idx, _ in all_scored_items_fallback[:5]]\n",
    "\n",
    "        # Pad with random unused indices\n",
    "        if len(top_indices) < 5 and chunk_indices:\n",
    "            used = set(top_indices)\n",
    "            available = [idx for idx in chunk_indices if idx not in used]\n",
    "            random.shuffle(available)\n",
    "            top_indices.extend(available[:5 - len(top_indices)])\n",
    "\n",
    "        # Cyclic duplicates if still short\n",
    "        while len(top_indices) < 5 and chunk_indices:\n",
    "            top_indices.append(chunk_indices[len(top_indices) % len(chunk_indices)])\n",
    "\n",
    "        # Absolute fallback\n",
    "        if len(top_indices) < 5:\n",
    "            top_indices = list(range(5))\n",
    "            print(f\"[ERROR] [{query_id}] Using default [0,1,2,3,4]\")\n",
    "\n",
    "        return (query_id, top_indices[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rank_all_documents(\n",
    "    df: pd.DataFrame,\n",
    "    client_stage1,\n",
    "    parser,\n",
    "    tracker,\n",
    "    rescue_client\n",
    ") -> List[Tuple[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    Process all document ranking queries.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame from load_document_data()\n",
    "        client_stage1: Stage 1 LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "        rescue_client: Rescue LLM client\n",
    "\n",
    "    Returns:\n",
    "        (query_id, ranking) tuples\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(QUERY_SEMAPHORE)\n",
    "\n",
    "    async def process_with_stagger(row, query_index):\n",
    "        initial_delay = query_index * DOC_STAGGER_INTERVAL\n",
    "        if initial_delay > 0:\n",
    "            await asyncio.sleep(initial_delay)\n",
    "        return await rank_single_document(row, semaphore, client_stage1, parser, tracker, rescue_client)\n",
    "\n",
    "    tasks = [process_with_stagger(row, idx) for idx, (_, row) in enumerate(df.iterrows())]\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"Document ranking\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def rank_all_chunks(\n",
    "    df: pd.DataFrame,\n",
    "    client_stage1,\n",
    "    client_stage2,\n",
    "    client_stage3,\n",
    "    parser,\n",
    "    tracker,\n",
    "    rescue_client,\n",
    "    local_predictions: List\n",
    ") -> List[Tuple[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    Process all chunk ranking queries.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame from load_chunk_data()\n",
    "        client_stage1: Stage 1 LLM client\n",
    "        client_stage2: Stage 2 LLM client\n",
    "        client_stage3: Stage 3 LLM client\n",
    "        parser: Response parser\n",
    "        tracker: API tracker\n",
    "        rescue_client: Rescue LLM client\n",
    "        local_predictions: List to store predictions\n",
    "\n",
    "    Returns:\n",
    "        (query_id, ranking) tuples\n",
    "    \"\"\"\n",
    "    query_semaphore = asyncio.Semaphore(QUERY_SEMAPHORE)\n",
    "    stage3_semaphore = asyncio.Semaphore(STAGE3_SEMAPHORE)\n",
    "\n",
    "    async def process_with_query_sem(row, query_index):\n",
    "        initial_delay = query_index * CHUNK_STAGGER_INTERVAL\n",
    "\n",
    "        if initial_delay > 0:\n",
    "            await asyncio.sleep(initial_delay)\n",
    "            print(f\"[Query {query_index}] Starting after {initial_delay:.1f}s stagger\")\n",
    "\n",
    "        async with query_semaphore:\n",
    "            return await rank_single_chunk(\n",
    "                row, stage3_semaphore,\n",
    "                client_stage1, client_stage2, client_stage3,\n",
    "                parser, tracker, rescue_client, local_predictions\n",
    "            )\n",
    "\n",
    "    tasks = [process_with_query_sem(row, idx) for idx, (_, row) in enumerate(df.iterrows())]\n",
    "    results = await tqdm_asyncio.gather(*tasks, desc=\"Chunk ranking\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(data_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Main execution pipeline.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing eval JSONL files\n",
    "        output_dir: Directory for output submission\n",
    "    \"\"\"\n",
    "    # Determine backends per stage\n",
    "    backend_stage1 = \"databricks\" if \"databricks\" in MODEL_STAGE1.lower() else \"openai\"\n",
    "    backend_stage2 = \"databricks\" if \"databricks\" in MODEL_STAGE2.lower() else \"openai\"\n",
    "    backend_stage3 = \"databricks\" if \"databricks\" in MODEL_STAGE3.lower() else \"openai\"\n",
    "\n",
    "    # Configure extra parameters for specific models\n",
    "    extra_params_stage1 = {}\n",
    "    if \"gpt-oss-120b\" in MODEL_STAGE1.lower():\n",
    "        extra_params_stage1[\"reasoning_effort\"] = \"medium\"\n",
    "    elif \"gpt-5\" in MODEL_STAGE1.lower():\n",
    "        extra_params_stage1[\"reasoning_effort\"] = \"minimal\"\n",
    "\n",
    "    client_stage1 = UnifiedLLMClient(\n",
    "        backend=backend_stage1,\n",
    "        model=MODEL_STAGE1,\n",
    "        temperature=0.1,\n",
    "        max_tokens=1500,\n",
    "        extra_params=extra_params_stage1\n",
    "    )\n",
    "\n",
    "    extra_params_stage2 = {}\n",
    "    if \"gpt-5\" in MODEL_STAGE2.lower():\n",
    "        extra_params_stage2[\"reasoning_effort\"] = \"medium\"\n",
    "\n",
    "    client_stage2 = UnifiedLLMClient(\n",
    "        backend=backend_stage2,\n",
    "        model=MODEL_STAGE2,\n",
    "        temperature=0.1,\n",
    "        max_tokens=1500,\n",
    "        extra_params=extra_params_stage2\n",
    "    )\n",
    "\n",
    "    extra_params_stage3 = {}\n",
    "    if \"gpt-5\" in MODEL_STAGE3.lower():\n",
    "        extra_params_stage3[\"reasoning_effort\"] = \"medium\"\n",
    "\n",
    "    client_stage3 = UnifiedLLMClient(\n",
    "        backend=backend_stage3,\n",
    "        model=MODEL_STAGE3,\n",
    "        temperature=0.1,\n",
    "        max_tokens=1500,\n",
    "        extra_params=extra_params_stage3\n",
    "    )\n",
    "\n",
    "    # Initialize rescue client if OpenAI API key available\n",
    "    rescue_client = None\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        rescue_client = UnifiedLLMClient(\n",
    "            backend=\"openai\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "    else:\n",
    "        print(\"[WARNING] OPENAI_API_KEY not set, rescue disabled\")\n",
    "\n",
    "    # Initialize tracker and parser\n",
    "    tracker = APITracker()\n",
    "    parser = ResponseParser()\n",
    "\n",
    "    # Initialize local predictions list\n",
    "    local_predictions = []\n",
    "\n",
    "    # Load data\n",
    "    doc_path = Path(data_dir) / \"document_ranking_kaggle_eval.jsonl\"\n",
    "    chunk_path = Path(data_dir) / \"chunk_ranking_kaggle_eval.jsonl\"\n",
    "\n",
    "    doc_df = load_document_data(doc_path)\n",
    "    chunk_df = load_chunk_data(chunk_path, TARGET_TOKENS_PER_PART)\n",
    "\n",
    "    # Apply sample size if specified\n",
    "    if SAMPLE_SIZE:\n",
    "        doc_df = doc_df.head(SAMPLE_SIZE)\n",
    "        chunk_df = chunk_df.head(SAMPLE_SIZE)\n",
    "\n",
    "    # Execute ranking\n",
    "    start_time = time.time()\n",
    "\n",
    "    doc_start = time.time()\n",
    "    doc_results = await rank_all_documents(doc_df, client_stage1, parser, tracker, rescue_client)\n",
    "    doc_time = time.time() - doc_start\n",
    "\n",
    "    chunk_start = time.time()\n",
    "    chunk_results = await rank_all_chunks(\n",
    "        chunk_df, client_stage1, client_stage2, client_stage3,\n",
    "        parser, tracker, rescue_client, local_predictions\n",
    "    )\n",
    "    chunk_time = time.time() - chunk_start\n",
    "\n",
    "    # Generate submission\n",
    "    submission_data = []\n",
    "    for query_id, ranking in doc_results:\n",
    "        if not ranking:\n",
    "            continue\n",
    "        sample_id = query_id if query_id.startswith('doc_') else f'doc_{query_id}'\n",
    "        for target_idx in ranking[:5]:\n",
    "            submission_data.append({'sample_id': sample_id, 'target_index': target_idx})\n",
    "\n",
    "    for query_id, ranking in chunk_results:\n",
    "        if not ranking:\n",
    "            continue\n",
    "        sample_id = query_id if query_id.startswith('chunk_') else f'chunk_{query_id}'\n",
    "        for target_idx in ranking[:5]:\n",
    "            submission_data.append({'sample_id': sample_id, 'target_index': target_idx})\n",
    "\n",
    "    # Write submission\n",
    "    output_path = Path(output_dir) / \"submission.csv\"\n",
    "\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['sample_id', 'target_index'])\n",
    "        for entry in submission_data:\n",
    "            writer.writerow([entry['sample_id'], entry['target_index']])\n",
    "\n",
    "    # Validate submission\n",
    "    validate_submission(output_path)\n",
    "\n",
    "    # Print summary\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    def format_time(seconds: float) -> str:\n",
    "        \"\"\"Format seconds as mm:ss\"\"\"\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}:{secs:02d}\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FUSION SE COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Submission: {output_path} ({len(submission_data)} rows)\")\n",
    "    print(f\"Runtime: {format_time(total_time)} (doc={format_time(doc_time)}, chunk={format_time(chunk_time)})\")\n",
    "    tracker.print_summary()\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: validate output for submission\n",
    "\n",
    "def validate_submission(csv_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Validate submission CSV format for Kaggle.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to submission.csv\n",
    "\n",
    "    Returns:\n",
    "        True if validation passed\n",
    "\n",
    "    Raises:\n",
    "        AssertionError if validation fails\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Check 1: Column names\n",
    "    expected_cols = ['sample_id', 'target_index']\n",
    "    assert list(df.columns) == expected_cols, f\"Expected columns {expected_cols}, got {list(df.columns)}\"\n",
    "\n",
    "    # Check 2: Row count (varies based on SAMPLE_SIZE)\n",
    "\n",
    "    # Check 3: No duplicates\n",
    "    assert not df.duplicated().any(), \"Found duplicate rows\"\n",
    "\n",
    "    # Check 4: Target index validity\n",
    "    doc_targets = df[df['sample_id'].str.startswith('doc_')]['target_index']\n",
    "    chunk_targets = df[df['sample_id'].str.startswith('chunk_')]['target_index']\n",
    "\n",
    "    assert doc_targets.between(0, 4).all(), \"Document target_index must be 0-4\"\n",
    "    assert chunk_targets.min() >= 0, \"Chunk target_index must be >= 0\"\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation\n",
    "- cost ~$2 USD in Databricks\n",
    "- for `gpt-oss-120b` and `llama-405b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document ranking: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]\n",
      "Chunk ranking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query 1] Starting after 3.0s stagger\n",
      "[WARNING] Attempt 1 failed: API error 429: {\"error_code\":\"REQUEST_LIMIT_EXCEEDED\",\"message\":\"REQUEST_LIMIT_EXCEEDED: Exceeded workspace input tokens per minute rate lim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk ranking:  50%|█████     | 1/2 [02:31<02:31, 151.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Attempt 1 failed: API error 429: {\"error_code\":\"REQUEST_LIMIT_EXCEEDED\",\"message\":\"REQUEST_LIMIT_EXCEEDED: Exceeded workspace input tokens per minute rate lim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk ranking: 100%|██████████| 2/2 [03:53<00:00, 116.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FUSION SE COMPLETE\n",
      "================================================================================\n",
      "Submission: output/submission.csv (20 rows)\n",
      "Runtime: 3:57 (doc=0:04, chunk=3:53)\n",
      "\n",
      "================================================================================\n",
      "API STATISTICS SUMMARY\n",
      "================================================================================\n",
      "Total API calls: 18\n",
      "Total time: 92.4s (avg: 5.14s/call)\n",
      "\n",
      "Document ranking:\n",
      "  Calls: 2\n",
      "  Time: 5.1s (avg: 2.55s/call)\n",
      "  Errors: 0\n",
      "  Retries: 0\n",
      "\n",
      "Chunk ranking:\n",
      "  Calls: 16\n",
      "  Time: 87.3s (avg: 5.46s/call)\n",
      "  Errors: 0\n",
      "  Retries: 0\n",
      "\n",
      "Retries breakdown:\n",
      "  Rate limit retries: 2\n",
      "  Parsing retries: 0\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the main execution pipeline\n",
    "# Notebook mode: Uses DATA_DIR and OUTPUT_DIR from Configuration cell above\n",
    "\n",
    "await main(DATA_DIR, OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag-challenge25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
